FROM gpt-oss:20b

# iGPU-optimized configuration for AMD Vulkan backend
# Addresses excessive token generation on short queries

# Context window - 8K tokens for RAG workloads
PARAMETER num_ctx 8192

# Output limit - prevents runaway generation (critical for short queries)
# Short queries were generating 2000+ tokens, causing 130s latency
PARAMETER num_predict 512

# Temperature - increased for reasoning models (per Ollama recommendations)
# Lower values (0.2) can cause empty content issues with gpt-oss
PARAMETER temperature 0.7

# Batch size optimized for iGPU memory bandwidth
PARAMETER num_batch 512

# Stop tokens to prevent generation beyond expected boundaries
PARAMETER stop "<|end|>"
PARAMETER stop "<|eot_id|>"
PARAMETER stop "<|im_end|>"

# NOTE: No custom TEMPLATE - use base model's Harmony format
# Custom templates break gpt-oss content generation
