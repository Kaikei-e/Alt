FROM gpt-oss:20b

# iGPU-optimized configuration for AMD Vulkan backend
# Addresses excessive token generation on short queries

# Context window - 8K tokens for RAG workloads
PARAMETER num_ctx 8192

# Output limit - increased for gpt-oss thinking + JSON output
# gpt-oss outputs thinking first, then JSON answer
# 512 was too short, causing truncation before JSON generation
PARAMETER num_predict 4096

# Temperature - increased for reasoning models (per Ollama recommendations)
# Lower values (0.2) can cause empty content issues with gpt-oss
PARAMETER temperature 0.7

# Batch size optimized for iGPU memory bandwidth
PARAMETER num_batch 512

# Stop tokens to prevent generation beyond expected boundaries
PARAMETER stop "<|end|>"
PARAMETER stop "<|eot_id|>"
PARAMETER stop "<|im_end|>"

# NOTE: No custom TEMPLATE - use base model's Harmony format
# Custom templates break gpt-oss content generation
