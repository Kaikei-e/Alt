# compose/recap.yaml - Recap services
# Article summarization and evaluation

include:
  - base.yaml

services:
  recap-db:
    build:
      context: ../docker/postgres
      dockerfile: Dockerfile.recap-db
    container_name: recap-db
    restart: always
    command: >
      postgres
      -c config_file=/etc/postgresql/postgresql.conf
      -c hba_file=/etc/postgresql/pg_hba.conf
    environment:
      POSTGRES_DB: ${RECAP_DB_NAME}
      POSTGRES_USER: ${RECAP_DB_USER}
      POSTGRES_PASSWORD_FILE: /run/secrets/recap_db_password
    secrets:
      - recap_db_password
    volumes:
      # PostgreSQL 18+ requires mount at /var/lib/postgresql (not /data)
      - recap_db_data:/var/lib/postgresql
      # Note: initスクリプトはAtlasマイグレーションに移行済みのため削除
      # - ../recap-worker/recap-db/init:/docker-entrypoint-initdb.d:rw
      - ../docker/postgres/postgresql-recap.conf:/etc/postgresql/postgresql.conf:ro
      - ../docker/postgres/pg_hba.conf:/etc/postgresql/pg_hba.conf:ro
    ports:
      - "5435:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${RECAP_DB_USER} -d ${RECAP_DB_NAME}"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - alt-network
    tty: true
    labels:
      - rask.group=recap-db
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  recap-db-migrator:
    build:
      context: ../recap-migration-atlas
      dockerfile: docker/Dockerfile
    container_name: recap_db_migrator
    command: ["apply"]
    environment:
      # Database credentials - password read from secret file
      RECAP_DB_HOST: recap-db
      RECAP_DB_PORT: ${RECAP_DB_PORT}
      RECAP_DB_USER: ${RECAP_DB_USER}
      RECAP_DB_NAME: ${RECAP_DB_NAME}
      RECAP_DB_PASSWORD_FILE: /run/secrets/recap_db_password
      ATLAS_ENV: ${ATLAS_ENV:-kubernetes}
      ATLAS_REVISIONS_SCHEMA: "public"
      MIGRATE_BASELINE_VERSION: ${RECAP_MIGRATE_BASELINE_VERSION:-}
    secrets:
      - recap_db_password
    volumes:
      - ../recap-migration-atlas/migrations:/migrations:ro
    depends_on:
      recap-db:
        condition: service_healthy
    networks:
      - alt-network

  recap-worker:
    env_file:
      - ../.env
    build:
      context: ../recap-worker
      dockerfile: Dockerfile.recap-worker
    networks:
      - alt-network
    environment:
      # Database credentials - password read from secret file (app constructs DSN)
      - RECAP_DB_HOST=recap-db
      - RECAP_DB_PORT=${RECAP_DB_PORT}
      - RECAP_DB_USER=${RECAP_DB_USER}
      - RECAP_DB_PASSWORD_FILE=/run/secrets/recap_db_password
      - RECAP_DB_NAME=${RECAP_DB_NAME}
      - NEWS_CREATOR_BASE_URL=http://news-creator:11434
      - SUBWORKER_BASE_URL=http://recap-subworker:8002
      - ALT_BACKEND_BASE_URL=${ALT_BACKEND_BASE_URL}
      - ALT_BACKEND_SERVICE_TOKEN_FILE=/run/secrets/service_secret
      - TAG_LABEL_GRAPH_WINDOW=${TAG_LABEL_GRAPH_WINDOW}
      - TAG_LABEL_GRAPH_TTL_SECONDS=${TAG_LABEL_GRAPH_TTL_SECONDS}
      - HUGGING_FACE_TOKEN_PATH=/run/secrets/hugging_face_token
      # HuggingFace cache directory (writable path for model downloads)
      - HF_HOME=/tmp/huggingface
      - TRANSFORMERS_CACHE=/tmp/huggingface/transformers
      # rust-bert cache directory (for embedding model)
      - RUSTBERT_CACHE=/tmp/rustbert
      # Clustering timeout: relax stuck detection from 600s to 1200s for large genres
      - RECAP_CLUSTERING_STUCK_THRESHOLD_SECS=${RECAP_CLUSTERING_STUCK_THRESHOLD_SECS:-1200}
      # Batch summary chunk size: reduced to 3 to prevent batch HTTP timeout
      # (10 genres × hierarchical Map-Reduce exceeded 1800s timeout)
      - RECAP_BATCH_SUMMARY_CHUNK_SIZE=${RECAP_BATCH_SUMMARY_CHUNK_SIZE:-3}
      # Per-genre LLM summary timeout (hierarchical Map-Reduce needs sufficient margin)
      - LLM_SUMMARY_TIMEOUT_SECS=${LLM_SUMMARY_TIMEOUT_SECS:-900}
      # Evening Pulse configuration
      - PULSE_ENABLED=true
      - PULSE_ROLLOUT_PERCENT=100
      - PULSE_VERSION=v4
    secrets:
      - recap_db_password
      - service_secret
      - hugging_face_token
    depends_on:
      recap-db:
        condition: service_healthy
    restart: unless-stopped
    ports:
      - "9005:9005"
    healthcheck:
      test: ["CMD", "/usr/local/bin/recap-worker", "healthcheck"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    labels:
      - rask.group=recap-worker
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  recap-subworker:
    env_file:
      - ../.env
    build:
      context: ../recap-subworker
      dockerfile: Dockerfile.recap-subworker
    networks:
      - alt-network
    environment:
      - RECAP_SUBWORKER_DB_PASSWORD_FILE=/run/secrets/recap_db_password
      - RECAP_SUBWORKER_BASE_URL=http://recap-subworker:8002
      - RECAP_SUBWORKER_MODEL_BACKEND=${RECAP_SUBWORKER_MODEL_BACKEND:-sentence-transformers}
      - RECAP_SUBWORKER_MODEL_ID=${RECAP_SUBWORKER_MODEL_ID:-intfloat/multilingual-e5-large}
      - RECAP_SUBWORKER_OLLAMA_EMBED_URL=${OLLAMA_EMBED_URL:-}
      - RECAP_SUBWORKER_OLLAMA_EMBED_MODEL=${OLLAMA_EMBED_MODEL:-mxbai-embed-large}
      - RECAP_SUBWORKER_PIPELINE_MODE=inprocess
      # Single worker for CUDA compatibility (CUDA context survives fork only in single worker mode)
      - RECAP_SUBWORKER_GUNICORN_WORKERS=1
      - RECAP_SUBWORKER_GUNICORN_WORKER_TIMEOUT=3600
      - RECAP_SUBWORKER_GUNICORN_MAX_REQUESTS=50000
      - RECAP_SUBWORKER_GUNICORN_GRACEFUL_TIMEOUT=120
      - RECAP_SUBWORKER_PROCESS_POOL_SIZE=2
      - RECAP_SUBWORKER_MAX_BACKGROUND_RUNS=2
      - RECAP_SUBWORKER_PIPELINE_WORKER_PROCESSES=2
      - RECAP_SUBWORKER_CLASSIFICATION_WORKER_PROCESSES=6
      - RECAP_SUBWORKER_CLASSIFICATION_POOL_IDLE_TIMEOUT_SECONDS=60
      # Embedding device (for sentence-transformers backend only, ignored when using ollama-remote)
      # - cuda: Use local NVIDIA GPU (requires workers=1 for CUDA compatibility)
      # - cpu: Use CPU (when using ollama-remote, set to cpu to avoid loading local GPU model)
      - RECAP_SUBWORKER_DEVICE=${RECAP_SUBWORKER_DEVICE:-cuda}
      # Increase batch size from 8 to 32 for better GPU utilization
      - RECAP_SUBWORKER_BATCH_SIZE=${RECAP_SUBWORKER_BATCH_SIZE:-32}
      # Reduce Optuna trials from 50 to 15 (TPE converges within 15-20 trials)
      - RECAP_SUBWORKER_BAYES_OPT_TRIALS=${RECAP_SUBWORKER_BAYES_OPT_TRIALS:-15}
      # Reduce UMAP dimensions from 20 to 8 for faster HDBSCAN
      - RECAP_SUBWORKER_UMAP_N_COMPONENTS=${RECAP_SUBWORKER_UMAP_N_COMPONENTS:-8}
      # HDBSCAN timeout before fallback to MiniBatchKMeans (seconds)
      - RECAP_SUBWORKER_HDBSCAN_TIMEOUT_SECS=${RECAP_SUBWORKER_HDBSCAN_TIMEOUT_SECS:-300}
      # Classification (Learning Machine) - CPU使用でOOM回避
      - RECAP_SUBWORKER_CLASSIFICATION_DEVICE=cpu
    ports:
      - "8002:8002"
    secrets:
      - recap_db_password
    volumes:
      - ../recap-subworker/data/golden_classification.json:/app/data/golden_classification.json:ro
      - ../recap-subworker/data/genre_classifier.joblib:/app/data/genre_classifier.joblib:ro
      - ../recap-subworker/data/genre_classifier_ja.joblib:/app/data/genre_classifier_ja.joblib:ro
      - ../recap-subworker/data/genre_classifier_en.joblib:/app/data/genre_classifier_en.joblib:ro
      - ../recap-subworker/data/tfidf_vectorizer.joblib:/app/data/tfidf_vectorizer.joblib:ro
      - ../recap-subworker/data/tfidf_vectorizer_ja.joblib:/app/data/tfidf_vectorizer_ja.joblib:ro
      - ../recap-subworker/data/tfidf_vectorizer_en.joblib:/app/data/tfidf_vectorizer_en.joblib:ro
      - ../recap-subworker/data/genre_thresholds.json:/app/data/genre_thresholds.json:ro
      - ../recap-subworker/data/genre_thresholds_ja.json:/app/data/genre_thresholds_ja.json:ro
      - ../recap-subworker/data/genre_thresholds_en.json:/app/data/genre_thresholds_en.json:ro
      # Learning Machine model artifacts
      - ../recap-subworker/recap_subworker/learning_machine/artifacts:/app/recap_subworker/learning_machine/artifacts:ro
    depends_on:
      recap-worker:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    labels:
      - rask.group=recap-subworker
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  dashboard:
    env_file:
      - ../.env
    build:
      context: ../dashboard
      dockerfile: Dockerfile
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, utility]
    ports:
      - "8501:8501"
      - "8502:8000"
    environment:
      # Database credentials - password read from secret file (app constructs DSN)
      - RECAP_DB_HOST=recap-db
      - RECAP_DB_PORT=5432
      - RECAP_DB_USER=${RECAP_DB_USER}
      - RECAP_DB_PASSWORD_FILE=/run/secrets/recap_db_password
      - RECAP_DB_NAME=${RECAP_DB_NAME}
      - SSE_PORT=8000
    secrets:
      - recap_db_password
    depends_on:
      recap-db:
        condition: service_healthy
    networks:
      - alt-network
    restart: unless-stopped
    labels:
      - rask.group=dashboard
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  recap-evaluator:
    build:
      context: ../recap-evaluator
      dockerfile: Dockerfile
    environment:
      # Database connection
      - RECAP_DB_DSN=postgres://${RECAP_DB_USER}:${RECAP_DB_PASSWORD}@recap-db:5432/${RECAP_DB_NAME}
      - OLLAMA_URL=http://news-creator:11434
      - OLLAMA_MODEL=gemma3-4b-8k
      - RECAP_WORKER_URL=http://recap-worker:9005
      - EVALUATION_SCHEDULE=0 6 * * *
      - LOG_LEVEL=INFO
      - LOG_FORMAT=json
    ports:
      - "8085:8080"
    depends_on:
      recap-db:
        condition: service_healthy
    networks:
      - alt-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    labels:
      - rask.group=recap-evaluator
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
