# compose/ai.yaml - AI/LLM services
# news-creator (FastAPI app), news-creator-backend (Ollama), pre-processor, redis-cache

include:
  - base.yaml

services:
  # Redis cache for news-creator recap summaries
  redis-cache:
    image: redis:8.0.2-alpine
    container_name: redis-cache
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis-cache-data:/data
    networks:
      - alt-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    labels:
      - rask.group=redis-cache
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Ollama server (separated from news-creator for faster deployments)
  news-creator-backend:
    container_name: news-creator-backend
    env_file:
      - ../.env
    build:
      context: ../news-creator
      dockerfile: Dockerfile.backend
    # GPU Configuration: runtime + deploy.resources for maximum compatibility
    # See: https://docs.docker.com/compose/how-tos/gpu-support/
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - LD_LIBRARY_PATH=/usr/lib/ollama/cuda_v12:/usr/lib/ollama/cuda_v13:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
      # Ollama 0.13.x CUDA ライブラリ選択バグの回避策 (GitHub Issue #13169)
      # OLLAMA_LLM_LIBRARY=cuda_v12 # Removed potentially conflicting override
      - OLLAMA_HOME=/home/ollama-user/.ollama
      - OLLAMA_MODELS=/home/ollama-user/.ollama
      - OLLAMA_HOST=0.0.0.0:11435
      # QAT model for improved quantization quality (54% less perplexity drop)
      # Set OLLAMA_BASE_MODEL=gemma3:4b for rollback
      - OLLAMA_BASE_MODEL=${OLLAMA_BASE_MODEL:-gemma3:4b-it-qat}
      # Optimization: Enable 2x Concurrency with 8k Context (Fits in 8GB VRAM)
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-2}
      # Strict limit to 1 model to prevent VRAM fragmentation/swapping
      - OLLAMA_MAX_LOADED_MODELS=${OLLAMA_MAX_LOADED_MODELS:-1}
      # Flash Attention 2.0: Disabled for stability
      # - OLLAMA_FLASH_ATTENTION=${OLLAMA_FLASH_ATTENTION:-1}
      # KV Cache quantization: Disabled (default f16) for stability
      # - OLLAMA_KV_CACHE_TYPE=${OLLAMA_KV_CACHE_TYPE:-q8_0}
    volumes:
      - news_creator_models:/home/ollama-user/.ollama
    restart: unless-stopped
    ports:
      - "11435:11435"
    networks:
      - alt-network
    depends_on:
      news-creator-volume-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11435/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    labels:
      - rask.group=news-creator-backend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # FastAPI application (lightweight, separated from Ollama)
  news-creator:
    container_name: news-creator
    env_file:
      - ../.env
    build:
      context: ../news-creator
      dockerfile: Dockerfile.app
    environment:
      - LLM_SERVICE_URL=http://news-creator-backend:11435
      - LLM_MODEL=gemma3-4b-8k
      - MODEL_ROUTING_ENABLED=false
      - LLM_KEEP_ALIVE_8K=24h
      - LLM_NUM_PREDICT=${LLM_NUM_PREDICT:-1200}
      - SUMMARY_NUM_PREDICT=${SUMMARY_NUM_PREDICT:-1200}
      - SERVICE_SECRET_FILE=/run/secrets/service_secret
      # Concurrency: sync with OLLAMA_NUM_PARALLEL in news-creator-backend
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-2}
      # LLM context window size (8k tokens)
      - LLM_NUM_CTX=${LLM_NUM_CTX:-8192}
      # 8K-only mode: disable 60K model, use hierarchical map-reduce for large documents
      - MODEL_60K_ENABLED=${MODEL_60K_ENABLED:-false}
      # Hierarchical summarization thresholds (optimized for 8k mode)
      - HIERARCHICAL_MAX_TOKENS=${HIERARCHICAL_MAX_TOKENS:-7000}
      - HIERARCHICAL_CHUNK_SIZE=${HIERARCHICAL_CHUNK_SIZE:-6000}
      # Cache settings
      - CACHE_ENABLED=${CACHE_ENABLED:-true}
      - CACHE_REDIS_URL=redis://redis-cache:6379/0
      - CACHE_TTL_SECONDS=${CACHE_TTL_SECONDS:-86400}
      # Hybrid RT/BE scheduling for TTFT optimization
      # RT slot reserved for streaming requests, BE slot for batch processing
      - SCHEDULING_RT_RESERVED_SLOTS=${SCHEDULING_RT_RESERVED_SLOTS:-1}
      - SCHEDULING_AGING_THRESHOLD_SECONDS=${SCHEDULING_AGING_THRESHOLD_SECONDS:-60}
      - SCHEDULING_PRIORITY_PROMOTION_THRESHOLD_SECONDS=${SCHEDULING_PRIORITY_PROMOTION_THRESHOLD_SECONDS:-120}
      - MAX_QUEUE_DEPTH=${MAX_QUEUE_DEPTH:-10}
      # OpenTelemetry
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://rask-log-aggregator:4318
      - OTEL_SERVICE_NAME=news-creator
    secrets:
      - service_secret
    restart: unless-stopped
    ports:
      - "11434:11434"
    networks:
      - alt-network
    depends_on:
      news-creator-backend:
        condition: service_healthy
      redis-cache:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    labels:
      - rask.group=news-creator
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  news-creator-volume-init:
    image: alpine:3.21
    entrypoint: ["/bin/sh", "-c"]
    command: ["chown -R 2000:2000 /home/ollama-user/.ollama"]
    user: "0:0"
    volumes:
      - news_creator_models:/home/ollama-user/.ollama
    networks:
      - alt-network
    restart: "no"
    labels:
      - rask.group=news-creator-init

  pre-processor:
    env_file:
      - ../.env
    build:
      context: ../pre-processor
      dockerfile: Dockerfile
    depends_on:
      redis-streams:
        condition: service_healthy
    environment:
      - DB_HOST=db
      - DB_PORT=5432
      - DB_NAME=${DB_NAME}
      - PRE_PROCESSOR_DB_USER=${PRE_PROCESSOR_DB_USER}
      - PRE_PROCESSOR_DB_PASSWORD_FILE=/run/secrets/pre_processor_db_password
      - DB_SSL_MODE=prefer
      - LOG_LEVEL=info
      - SERVICE_NAME=pre-processor
      - REDIS_STREAMS_URL=redis://redis-streams:6379
      - CONSUMER_ENABLED=true
      - CONSUMER_GROUP=pre-processor-group
      - CONSUMER_NAME=pre-processor-1
      # OpenTelemetry
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://rask-log-aggregator:4318
      - OTEL_SERVICE_NAME=pre-processor
    secrets:
      - pre_processor_db_password
    restart: always
    ports:
      - "9200:9200" # REST API
      - "9202:9202" # Connect-RPC
    networks:
      - alt-network
    healthcheck:
      test: ["CMD", "/pre-processor", "healthcheck"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    tty: true
    labels:
      - rask.group=pre-processor
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  redis-cache-data:
