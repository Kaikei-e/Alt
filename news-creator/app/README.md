# News Creator Service

LLM-based content generation service for the Alt RSS Reader project. Generates Japanese summaries and derivative content from English articles using local LLM (Ollama/Gemma).

## üèóÔ∏è Architecture

This service follows **Clean Architecture** principles with a 5-layer design:

```
Handler ‚Üí Usecase ‚Üí Port ‚Üí Gateway ‚Üí Driver
```

- **Handler**: REST API endpoints (FastAPI)
- **Usecase**: Business logic orchestration
- **Port**: Abstract interfaces for external dependencies
- **Gateway**: Anti-Corruption Layer for external services
- **Driver**: HTTP clients for external APIs

For detailed architecture documentation, see [CLAUDE.md](CLAUDE.md).

## üöÄ Quick Start

### Prerequisites

- Python 3.11+
- Ollama running locally or accessible via network
- Service secret key

### Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Set required environment variables
export SERVICE_SECRET=your-secret-key
export LLM_SERVICE_URL=http://localhost:11434
export LLM_MODEL=gemma3:4b

# Run the service
python main.py
```

The service will start on `http://localhost:8001`.

### Docker

```bash
# Build image
docker build -t news-creator:latest .

# Run container
docker run -p 8001:8001 \
  -e SERVICE_SECRET=your-secret-key \
  -e LLM_SERVICE_URL=http://host.docker.internal:11434 \
  news-creator:latest
```

## üìù API Usage

### Generate Japanese Summary

```bash
curl -X POST http://localhost:8001/api/v1/summarize \
  -H "Content-Type: application/json" \
  -d '{
    "article_id": "article-123",
    "content": "Full article text in English..."
  }'
```

**Response:**
```json
{
  "success": true,
  "article_id": "article-123",
  "summary": "Êó•Êú¨Ë™û„ÅÆË¶ÅÁ¥Ñ...",
  "model": "gemma3:4b",
  "prompt_tokens": 1234,
  "completion_tokens": 456,
  "total_duration_ms": 1500.5
}
```

### Generic LLM Generation (Ollama-compatible)

```bash
curl -X POST http://localhost:8001/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Your prompt here",
    "model": "gemma3:4b",
    "options": {
      "temperature": 0.7
    }
  }'
```

### Health Check

```bash
curl http://localhost:8001/health
```

## ‚öôÔ∏è Configuration

All configuration is done via environment variables:

### Required

| Variable | Description | Example |
|----------|-------------|---------|
| `SERVICE_SECRET` | Service authentication secret | `your-secret-key` |

### LLM Service

| Variable | Description | Default |
|----------|-------------|---------|
| `LLM_SERVICE_URL` | Ollama service URL | `http://localhost:11434` |
| `LLM_MODEL` | Model name | `gemma3:4b` |
| `LLM_TIMEOUT_SECONDS` | Request timeout | `60` |
| `LLM_KEEP_ALIVE_SECONDS` | Model keep-alive | `-1` (forever) |

### LLM Parameters

| Variable | Description | Default |
|----------|-------------|---------|
| `LLM_TEMPERATURE` | Generation temperature | `0.0` |
| `LLM_TOP_P` | Nucleus sampling parameter | `0.9` |
| `LLM_NUM_PREDICT` | Max tokens to generate | `500` |
| `LLM_REPEAT_PENALTY` | Repetition penalty | `1.0` |
| `LLM_NUM_CTX` | Context window size | `8192` |
| `LLM_STOP_TOKENS` | Stop tokens (comma-separated) | `<\|user\|>,<\|system\|>` |
| `SUMMARY_NUM_PREDICT` | Max tokens for summaries | `500` |

### Authentication (Optional)

| Variable | Description | Default |
|----------|-------------|---------|
| `AUTH_SERVICE_URL` | Auth service URL | `http://auth-service:8080` |

## üß™ Testing

### Run All Tests

```bash
SERVICE_SECRET=test-secret pytest
```

### Run Tests by Layer

```bash
# Config layer
SERVICE_SECRET=test-secret pytest tests/config/

# Domain layer
SERVICE_SECRET=test-secret pytest tests/domain/

# Usecase layer
SERVICE_SECRET=test-secret pytest tests/usecase/

# Handler layer
SERVICE_SECRET=test-secret pytest tests/handler/
```

### Coverage Report

```bash
SERVICE_SECRET=test-secret pytest --cov=news_creator --cov-report=html
```

## üìÅ Project Structure

```
news-creator/app/
‚îú‚îÄ‚îÄ main.py                          # FastAPI app + DI Container
‚îú‚îÄ‚îÄ requirements.txt                 # Python dependencies
‚îú‚îÄ‚îÄ CLAUDE.md                        # Detailed architecture docs
‚îú‚îÄ‚îÄ README.md                        # This file
‚îú‚îÄ‚îÄ news_creator/                    # Main package
‚îÇ   ‚îú‚îÄ‚îÄ config/                      # Configuration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.py
‚îÇ   ‚îú‚îÄ‚îÄ domain/                      # Domain models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompts.py
‚îÇ   ‚îú‚îÄ‚îÄ port/                        # Port interfaces
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_provider_port.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth_port.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ user_preferences_port.py
‚îÇ   ‚îú‚îÄ‚îÄ driver/                      # External API clients
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ollama_driver.py
‚îÇ   ‚îú‚îÄ‚îÄ gateway/                     # Anti-Corruption Layer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ollama_gateway.py
‚îÇ   ‚îú‚îÄ‚îÄ usecase/                     # Business logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ summarize_usecase.py
‚îÇ   ‚îî‚îÄ‚îÄ handler/                     # REST endpoints
‚îÇ       ‚îú‚îÄ‚îÄ summarize_handler.py
‚îÇ       ‚îú‚îÄ‚îÄ generate_handler.py
‚îÇ       ‚îî‚îÄ‚îÄ health_handler.py
‚îî‚îÄ‚îÄ tests/                           # Test suite
    ‚îú‚îÄ‚îÄ config/
    ‚îú‚îÄ‚îÄ domain/
    ‚îú‚îÄ‚îÄ driver/
    ‚îú‚îÄ‚îÄ gateway/
    ‚îú‚îÄ‚îÄ usecase/
    ‚îî‚îÄ‚îÄ handler/
```

## üõ†Ô∏è Development

### Adding a New Feature

1. **Write tests first** (TDD approach)
2. Start from the **Handler** layer
3. Work inward through **Usecase**, **Gateway**, **Driver**
4. Update **DependencyContainer** in `main.py`

### Adding a New External Service

1. Define a **Port** interface in `port/`
2. Implement the **Driver** in `driver/`
3. Implement the **Gateway** in `gateway/`
4. Use the Port in **Usecase**
5. Wire in **DependencyContainer**

See [CLAUDE.md](CLAUDE.md) for detailed development guidelines.

## üîí Security

### LLM Security Testing

This service includes security testing for LLM-specific vulnerabilities:

- **Prompt Injection**: Testing against adversarial prompts
- **Output Sanitization**: Validating LLM outputs before use
- **Information Disclosure**: Preventing leakage of sensitive data

### OWASP Top 10 for LLM Applications

We follow the [OWASP Top 10 for LLM Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/) security guidelines.

## üìö Related Documentation

- [CLAUDE.md](CLAUDE.md) - Detailed architecture and development guide
- [Alt Project README](../../README.md) - Main project documentation
- [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Ollama Documentation](https://github.com/ollama/ollama)

## üèÉ Running in Production

### Environment Setup

```bash
# Create .env file
cat > .env << EOF
SERVICE_SECRET=$(openssl rand -hex 32)
LLM_SERVICE_URL=http://ollama-service:11434
LLM_MODEL=gemma3:4b
LLM_TIMEOUT_SECONDS=120
AUTH_SERVICE_URL=http://auth-service:8080
EOF

# Load environment
source .env

# Run with gunicorn
gunicorn main:app \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker \
  --bind 0.0.0.0:8001
```

### Health Monitoring

```bash
# Check service health
curl http://localhost:8001/health

# Expected response
{"status":"healthy","service":"news-creator"}
```

## ü§ù Contributing

1. Follow TDD principles
2. Write tests for each layer
3. Maintain Clean Architecture boundaries
4. Update documentation
5. Run all tests before committing

## üìÑ License

Part of the Alt RSS Reader project.

## üêõ Troubleshooting

### LLM Connection Issues

```bash
# Check Ollama is running
curl http://localhost:11434/api/tags

# Test LLM directly
curl http://localhost:11434/api/generate -d '{
  "model": "gemma3:4b",
  "prompt": "Hello"
}'
```

### Import Errors

```bash
# Ensure news_creator package is in PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# Or run from the app directory
cd /path/to/news-creator/app
python main.py
```

### Test Failures

```bash
# Ensure SERVICE_SECRET is set
export SERVICE_SECRET=test-secret

# Run with verbose output
pytest -v -s

# Check specific test
pytest tests/usecase/test_summarize_usecase.py -v
```

## üìä Performance

- **Average summary generation**: ~1-3 seconds
- **Max tokens**: 1500 characters (enforced)
- **Concurrent requests**: Supports async operations
- **Memory usage**: ~200MB base + model memory

## üîÑ Version History

### v2.0.0 (Current)
- **Major refactoring** to Clean Architecture
- 5-layer design with strict separation of concerns
- Improved testability with dependency injection
- Better error handling and logging

### v1.0.0
- Initial monolithic implementation
- Basic summarization functionality
