# Fix FetchArticleContent 502 Timeout via Connect-Timeout-Ms Semantics

## ADR's STATUS

Accepted — 2026-02-27

## CONTEXT

`FetchArticleContent` API calls from the browser consistently returned HTTP 502 after exactly 30 seconds. The root cause was a timeout hierarchy conflict in the BFF (Backend-For-Frontend) service.

**Observed symptoms:**

| Layer | Evidence |
|-------|----------|
| nginx | All requests showed `rt=30.00x` with 502 |
| BFF | `context deadline exceeded (Client.Timeout exceeded while awaiting headers)` |
| Backend | `rate limit wait failed: context canceled` (context cancellation propagated from BFF) |

The backend itself was functioning correctly, but the article content fetch workflow — rate limiter wait (5s) + robots.txt check + HTTP fetch (10–30s) — routinely exceeded 30 seconds for uncached articles.

**Root cause:** The BFF's `http.Client` had `Timeout: 30s` hardcoded. Even though the frontend sent `Connect-Timeout-Ms: 120000` to request a 2-minute timeout, Go's `http.Client.Timeout` fired first and cancelled the request. In Connect-RPC semantics, the **client** controls timeout via `Connect-Timeout-Ms` header, not the proxy.

## DECISION MAKING

### Approach: Per-Request Context Deadline from Connect-Timeout-Ms

Remove the global `http.Client.Timeout` and instead derive a per-request `context.WithTimeout` from the `Connect-Timeout-Ms` header. This respects Connect-RPC's timeout propagation semantics where the originating client decides how long to wait.

### Timeout Resolution Logic

```
1. Parse Connect-Timeout-Ms header
2. If valid and > 0 → use as timeout
3. If absent or invalid → use defaultTimeout (30s, configurable)
4. Cap at maxConnectTimeout (5 minutes) to prevent abuse
5. Apply as context deadline on the request
```

The streaming client retains its own `http.Client.Timeout` as a safety net since streaming has different lifecycle characteristics.

### Files Modified

| File | Change |
|------|--------|
| `alt-butterfly-facade/internal/client/backend_client.go` | Removed `httpClient.Timeout`, added `DefaultTimeout()` getter |
| `alt-butterfly-facade/internal/handler/proxy_handler.go` | Added `defaultTimeout` field, `applyConnectTimeout()` method in `ServeHTTP` |
| `alt-butterfly-facade/internal/handler/bff_handler.go` | Same `applyConnectTimeout()` pattern for BFF-featured handler |
| `alt-butterfly-facade/internal/server/server.go` | Pass configured timeout to handler constructors |
| `alt-frontend-sv/src/lib/connect/articles.ts` | Set `timeoutMs: 120_000` for `fetchArticleContent` |

### Tests Added

| Test | Validates |
|------|-----------|
| `TestBackendClient_HttpClientTimeout_IsZero` | `httpClient.Timeout` is disabled (0) |
| `TestBackendClient_DefaultTimeout` | Getter returns configured value |
| `TestProxyHandler_ApplyConnectTimeout_WithHeader` | 120000ms header → 120s context deadline |
| `TestProxyHandler_ApplyConnectTimeout_WithoutHeader` | Missing header → 30s default |
| `TestProxyHandler_ApplyConnectTimeout_CappedAt5Minutes` | Excessive value → 5min cap |
| `TestBFFHandler_ApplyConnectTimeout_*` | Same 3 scenarios for BFF handler |

## RESULTS, EFFECTS

### Request Flow (After)

```
Frontend                    BFF                         Backend
   |                         |                            |
   |-- Connect-Timeout-Ms ---|                            |
   |   120000                |                            |
   |                         |-- ctx deadline: 120s ----->|
   |                         |                            |-- rate limit (5s)
   |                         |                            |-- robots.txt
   |                         |                            |-- HTTP fetch (10-30s)
   |                         |<--- response --------------|
   |<--- response -----------|                            |
```

Previously, the BFF's 30s `http.Client.Timeout` would fire at the dashed line, cancelling the backend's in-progress work. Now the 120s context deadline gives the backend sufficient time.

### PROS

- Respects Connect-RPC's timeout propagation semantics — client controls timeout
- Per-request granularity: long-running endpoints (FetchArticleContent) can request more time without affecting others
- 5-minute cap prevents unbounded resource holding
- Existing endpoints with no `Connect-Timeout-Ms` header fall back to previous 30s behavior — zero behavioral change for unmodified clients
- Streaming client timeout is unaffected

### CONS, TRADEOFF

- The BFF no longer enforces a global hard timeout on unary requests. A misbehaving client could hold connections for up to 5 minutes (the cap). This is acceptable given the BFF is an internal service behind nginx, which has its own timeout controls.
- `FetchArticleContent` now has a 120s client timeout. If the backend is truly unresponsive, the user waits longer before seeing an error. This is the intended tradeoff — waiting 2 minutes for article content is preferable to always failing at 30s.

## APPENDIX

- Connect-RPC Timeout specification: https://connectrpc.com/docs/protocol/#unary-request
- Go `http.Client.Timeout` vs context deadline: `http.Client.Timeout` applies to the entire request lifecycle and cannot be overridden per-request. Context deadlines are composable and propagate through the call chain.
