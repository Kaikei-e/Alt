# Tag Generator メモリ最適化: FP16 サポート

## ステータス

**承認済み（Accepted）** - 2026年1月

## コンテキスト

### 背景

Tag Generator サービスは、SentenceTransformer + KeyBERT を用いた ML ベースのタグ抽出を行う Python サービスである。本番環境でメモリ使用量が約 2GB に達し、リソース効率の改善が求められていた。

### 既存アーキテクチャ

Tag Generator のモデル管理は以下の設計思想に基づいている：

#### 1. シングルトン ModelManager (`model_manager.py`)

```python
class ModelManager:
    _instance: "ModelManager | None" = None
    _lock: ClassVar[threading.Lock] = threading.Lock()

    def __new__(cls) -> "ModelManager":
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
```

**設計意図:**
- プロセス内でモデルを一度だけロードし、メモリを節約
- スレッドセーフなダブルチェックロッキングパターン
- 複数の `TagExtractor` インスタンスがモデルを共有

#### 2. ONNX Runtime フォールバック

```python
if use_onnx:
    self._embedder = OnnxEmbeddingModel(onnx_config)
else:
    self._embedder = SentenceTransformer(config.model_name, device=config.device)
```

**設計意図:**
- ONNX モデルファイルが存在すれば ONNX Runtime を使用（推論高速化）
- 存在しなければ SentenceTransformer + KeyBERT にフォールバック
- `__post_init__` でモデルファイル存在チェックを自動実行

#### 3. 遅延ロード (Lazy Loading)

```python
def _lazy_load_models(self) -> None:
    if not self._models_loaded:
        model_config = ModelConfig(...)
        self._embedder, self._keybert, self._ja_tagger = self._model_manager.get_models(model_config)
        self._models_loaded = True
```

**設計意図:**
- サービス起動時ではなく、最初のタグ抽出リクエスト時にモデルをロード
- コールドスタート時間の短縮

### メモリ使用量の内訳（改修前）

| コンポーネント | 推定サイズ | 備考 |
|----------------|-----------|------|
| PyTorch ランタイム | 400-500MB | ライブラリ自体のオーバーヘッド |
| SentenceTransformer 重み | 400-600MB | `paraphrase-multilingual-MiniLM-L12-v2` |
| KeyBERT | 200-300MB | Embedder への参照を保持 |
| Fugashi (日本語形態素解析) | 50-100MB | unidic 辞書含む |
| NLTK Stopwords | 10-20MB | 日英ストップワード |
| **合計** | **~1.5-2GB** | |

### 検討した代替案

1. **より小さいモデルへの変更** (`all-MiniLM-L6-v2`)
   - 却下理由: 多言語対応が必要、精度トレードオフが不明確

2. **LazyModelManager の LRU eviction**
   - 却下理由: 単一モデルを継続使用するため、eviction のメリットが薄い

3. **INT8 量子化**
   - 保留: FP16 より実装が複雑、追加の量子化ステップが必要

## 意思決定

### 決定: FP16 (半精度浮動小数点) サポートの追加

**変更前:**
```python
self._embedder = SentenceTransformer(config.model_name, device=config.device)
```

**変更後:**
```python
self._embedder = SentenceTransformer(config.model_name, device=config.device)

if config.use_fp16:
    try:
        self._embedder.half()  # FP32 → FP16
        logger.info("FP16 conversion applied", expected_memory_reduction="~50%")
    except Exception as fp16_error:
        logger.warning("FP16 conversion failed, continuing with FP32", error=str(fp16_error))
```

**理由:**
- SentenceTransformer のモデル重みを 32-bit から 16-bit に変換
- 理論上 50% のメモリ削減（400-600MB → 200-300MB）
- 推論精度への影響は最小限（埋め込みベクトルの用途では許容範囲）

### 設定インターフェース

#### 環境変数

```bash
# FP16 を有効化
TAG_USE_FP16=true
```

#### コード上の設定

```python
@dataclass
class ModelConfig:
    use_fp16: bool = False  # Enable FP16 for ~50% memory reduction

@dataclass
class TagExtractionConfig:
    use_fp16: bool = False  # Set via TAG_USE_FP16=true

    def __post_init__(self) -> None:
        if os.getenv("TAG_USE_FP16", "").lower() in ("true", "1", "yes"):
            self.use_fp16 = True
```

### 修正ファイル

| ファイル | 変更内容 |
|----------|----------|
| `tag_extractor/model_manager.py` | `ModelConfig.use_fp16` 追加、`_load_models()` で FP16 変換 |
| `tag_extractor/extract.py` | `TagExtractionConfig.use_fp16` 追加、環境変数対応 |

## 結果

### 期待されるメモリ削減

| 設定 | メモリ使用量 | 削減量 |
|------|-------------|--------|
| FP32 (デフォルト) | ~2GB | - |
| FP16 有効化 | ~1.6-1.8GB | ~200-400MB |
| FP16 + ONNX (将来) | ~1.4-1.6GB | ~400-600MB |

### PROS

1. **後方互換性**
   - デフォルト無効、環境変数でオプトイン
   - 既存のデプロイに影響なし

2. **シンプルな実装**
   - PyTorch の `.half()` メソッドを利用
   - 追加のモデル変換ステップ不要

3. **観測可能性**
   - `embedder_metadata["fp16"]` でランタイム確認可能
   - ログに FP16 適用状況を出力

4. **段階的ロールアウト**
   - 環境変数で制御可能
   - 問題発生時は無効化で即時ロールバック

### CONS, TRADEOFF

1. **CPU での推論速度低下の可能性**
   - CPU は FP16 ネイティブサポートなし
   - 内部で FP32 に変換されるため、速度向上は期待できない
   - **対策**: GPU 環境での使用を推奨

2. **数値精度の微小な低下**
   - 埋め込みベクトルの精度が若干低下
   - **影響**: タグ抽出の用途では実用上無視可能

3. **ONNX Runtime との非互換**
   - ONNX 使用時は FP16 変換不要（ONNX 側で最適化済み）
   - **対策**: ONNX 有効時は FP16 設定を無視

## 付録

### 関連する既存設計

- **ONNX Runtime**: `onnx_embedder.py` で実装済み、`TAG_ONNX_MODEL_PATH` で設定
- **GC 制御**: `BatchProcessor` で `memory_cleanup_interval` ごとに `gc.collect()` 実行
- **シングルトンパターン**: `ModelManager` でプロセス内モデル共有

### 今後の改善案

1. **ONNX モデル変換パイプライン**
   - Docker ビルド時に SentenceTransformer → ONNX 変換
   - 推論速度向上 + メモリ削減

2. **INT8 量子化**
   - FP16 より大きなメモリ削減（理論上 75%）
   - 精度影響の評価が必要

3. **動的バッチサイズ調整**
   - メモリ使用量に応じてバッチサイズを自動調整

### 参考資料

- [Sentence Transformers Memory Reduction](https://milvus.io/ai-quick-reference/how-can-you-reduce-the-memory-footprint-of-sentence-transformer-models-during-inference-or-when-handling-large-numbers-of-embeddings)
- [PyTorch FP16 Inference](https://pytorch.org/docs/stable/amp.html)
- [FastAPI ML Model Serving](https://luis-sena.medium.com/how-to-optimize-fastapi-for-ml-model-serving-6f75fb9e040d)
