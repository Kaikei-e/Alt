# RAG生成プロセスの再設計と簡素化 (ADR 000002)

## ステータス

採択（Accepted）

## コンテキスト

ADR 000001 で導入された「二段階 LLM 呼び出し (Stage 1: Citations, Stage 2: Answer)」および「厳格な JSON Schema」において、以下の重大な問題が判明した。

1.  **"Insufficient Information" の多発**:
    *   Stage 1 で引用句 (Quotes) を抽出させた際、LLM が文脈内に明確な答えを見つけられない（あるいは引用形式に固執しすぎる）と判断すると、Stage 2 に渡される情報が欠落する、または Stage 2 自体が「情報なし」と判断するケースが多発している。
    *   特に、JSON Schema で `quotes` フィールドを必須としているにもかかわらず、プロンプトで「引用がない場合は空配列」と指示するなど、Schema と Prompt の矛盾が LLM の混乱（ハルシネーションや拒否）を招いている可能性がある。

2.  **レイテンシの悪化**:
    *   2回の LLM 往復 (Round Trip) が発生するため、Time To First Token (TTFT) が遅延する。
    *   特に小規模モデル (20B以下) や CPU 推論環境ではこの遅延が顕著である。

3.  **実装の複雑化**:
    *   2つの異なるプロンプト、中間データの引き継ぎ、それぞれのバリデーションなど、コードベースが不必要に複雑化している。

## 決定

RAG 生成プロセスを **「シングルフェーズ (Single-Phase)」** に戻し、プロンプトとスキーマを劇的に簡素化する。

### 1. シングルフェーズへの回帰と検索件数の増加

*   **変更**: `citations` 抽出と `answer` 生成を1回の LLM 呼び出しで行う。
*   **変更**: Vector Search の取得件数 (`searchLimit`) を **5件から10件** に増加させる。
*   **理由**: 最近の LLM は、適切な指示を与えれば、回答生成と同時に引用を行う能力を十分に持っている。コンテキストを一度渡すだけで済むため、レイテンシが大幅に改善される。また、コンテキストウィンドウの拡大に伴い、より多くの検索結果（10件）を渡すことで、回答の網羅性と精度を向上させる。

### 2. JSON Schema の簡素化 (Quotes の廃止)

*   **変更**: 出力フォーマットの `quotes` (原文の抜粋) 配列を廃止し、`answer` (回答本文) と `citations` (参照メタデータ) のみに絞る。
*   **理由**: `quotes` の抽出は LLM にとって負荷が高く、また「回答に使った部分」と「回答そのもの」の重複を生む。`answer` 内で `[chunk_id]` 形式の参照を行い、後処理で `citations` 配列と紐付けるだけで十分である。
*   **補足**: OpenAI 等のベストプラクティス調査に基づき、LLM には最小限の識別子 (`chunk_id`) のみを出力させ、URL や Title などのメタデータはアプリケーション側で結合（Hydration）する方式を採用する。これにより、トークン消費量を抑えつつ、ハルシネーションのリスクを低減する。
*   **新スキーマ案**:
    ```json
    {
      "type": "object",
      "properties": {
        "answer": { "type": "string" },
        "citations": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "chunk_id": { "type": "string" },
              "reason": { "type": "string" } // 任意: なぜこのチャンクを引用したか
            },
            "required": ["chunk_id"]
          }
        },
        "fallback": { "type": "boolean" },
        "reason": { "type": "string" }
      },
      "required": ["answer", "citations", "fallback", "reason"]
    }
    ```

### 3. プロンプトのストレート化

*   **変更**: System Prompt を「あなたは与えられた Context のみを用いて質問に回答するAIです」というシンプルなものにする。
*   **指示**:
    1.  コンテキスト内の情報の事実関係のみを使うこと。
    2.  知識がない場合は素直に `fallback: true` を返すこと。
    3.  Schema 構造に従うこと。

## 結果・影響

### 利点

*   **安定性向上**: 「引用抽出」という中間タスクがなくなるため、LLM が回答生成に集中でき、「情報なし」という誤判定が減る。
*   **高速化**: LLM 呼び出しが1回になるため、応答速度が倍近く向上する可能性がある。
*   **保守性**: `answer_with_rag_usecase.go` のロジックが半分以下になり、バグの温床が減る。

### 注意点

*   回答内の引用精度（どの文がどのチャンクに基づいているか）は、LLM の性能に直接依存するようになる（中間チェックがないため）。しかし、これは現状の技術レベルでは許容範囲である。

## 補足: 実装計画

本 ADR 採択後、直ちに `rag-orchestrator` の改修を行う。ADR 000001 の変更点は実質的にロールバック再構築となる。
