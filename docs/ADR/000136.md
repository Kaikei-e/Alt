# news-creator: Gemma3 QAT モデルへの移行

## ADR's STATUS

Accepted

## CONTEXT

### 背景

news-creator-backend で使用している LLM モデル `gemma3:4b`（標準 Q4_K_M 量子化）を、より高品質な量子化手法である QAT（Quantization-Aware Training）版 `gemma3:4b-it-qat` へ移行する。

### QAT モデルの特徴

QAT（Quantization-Aware Training）は、量子化を学習プロセスに組み込むことで、量子化後の品質劣化を最小限に抑える手法。

| 項目 | 標準量子化 (Q4_K_M) | QAT 量子化 |
|------|---------------------|------------|
| モデルサイズ | ~3.3GB | ~2.6GB |
| Perplexity 低下 | 基準 | 54% 改善 |
| BF16 との品質差 | 大きい | 小さい（同等に近い） |

### 参考

- [Google Developers Blog - Gemma 3 QAT](https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/)

## DECISION MAKING

### 修正対象ファイル

| ファイル | 変更内容 |
|----------|----------|
| `news-creator/entrypoint-backend.sh` | ベースモデル名を環境変数化、デフォルトを QAT に変更 |
| `news-creator/Modelfile.gemma3-4b-16k` | `FROM gemma3:4b` → `FROM gemma3:4b-it-qat` |
| `news-creator/Modelfile.gemma3-4b-60k` | `FROM gemma3:4b` → `FROM gemma3:4b-it-qat` |
| `news-creator/app/news_creator/config/config.py` | デフォルトモデル名を QAT に変更 |
| `compose/ai.yaml` | `OLLAMA_BASE_MODEL` 環境変数追加 |
| `news-creator/app/tests/config/test_config.py` | テストのアサーションを更新 |

### ロールバック対応

環境変数 `OLLAMA_BASE_MODEL` でモデルを切り替え可能にし、問題発生時の即時ロールバックをサポート:

```bash
# ロールバック時
export OLLAMA_BASE_MODEL=gemma3:4b
docker compose -f compose/compose.yaml up -d news-creator-backend
```

### チューニングパラメータ

既存パラメータを維持（保守的アプローチ）:

| パラメータ | 値 | 備考 |
|-----------|-----|------|
| temperature | 0.15 | 変更なし |
| top_p | 0.85 | 変更なし |
| top_k | 40 | 変更なし |
| repeat_penalty | 1.15 | 変更なし |
| num_batch | 1024 | RTX 4060 最適化 |

## RESULTS, EFFECTS

### 変更内容

#### entrypoint-backend.sh

```bash
# Before
ollama pull gemma3:4b

# After
BASE_MODEL="${OLLAMA_BASE_MODEL:-gemma3:4b-it-qat}"
ollama pull "${BASE_MODEL}"
```

#### Modelfile.gemma3-4b-16k / 60k

```diff
-FROM gemma3:4b
+FROM gemma3:4b-it-qat
 PARAMETER num_ctx 16384
```

#### config.py

```diff
-self.model_name = os.getenv("LLM_MODEL", "gemma3:4b")
+self.model_name = os.getenv("LLM_MODEL", "gemma3:4b-it-qat")

-self.model_base_name = os.getenv("MODEL_BASE_NAME", "gemma3:4b")
+self.model_base_name = os.getenv("MODEL_BASE_NAME", "gemma3:4b-it-qat")
```

### 検証結果

- ユニットテスト: 93 件すべてパス
- コンテナヘルスチェック: healthy

### パフォーマンス検証結果（2026-01-21）

#### 検証環境

- **GPU**: NVIDIA GeForce RTX 4060 (8GB VRAM)
- **VRAM 使用量**: 5,935 MiB（モデルロード後）
- **温度**: 50°C（アイドル時）

#### 16K コンテキストモデル（gemma3-4b-16k）

| ケース | TTFT P50 | Decode P50 | Prefill P50 | 結果 |
|--------|----------|------------|-------------|------|
| small (~2,000 chars) | 0.31s | 52.7 tok/s | 4,380 tok/s | ✅ PASS |
| medium (~10,000 chars) | 0.57s | 52.1 tok/s | 4,056 tok/s | ✅ PASS |
| large (~50,000 chars) | 1.13s | 57.5 tok/s | 4,569 tok/s | ✅ PASS |

**目標値との比較**:
- TTFT: < 2s → **達成** (0.31s - 1.13s)
- Decode: > 40 tok/s → **達成** (52.1 - 57.5 tok/s)
- Prefill: > 500 tok/s → **達成** (4,056 - 4,569 tok/s)

#### 60K コンテキストモデル（gemma3-4b-60k）

| ケース | TTFT P50 | Decode P50 | Prefill P50 | 結果 |
|--------|----------|------------|-------------|------|
| xl (~150,000 chars) | 20.96s | 26.4 tok/s | 4,614 tok/s | ⚠️ 制限付き |

**備考**: 60K 以上のコンテキストは RTX 4060 (8GB) では VRAM 制約により実用的ではない（ADR-043 と一致）。大規模入力には Map-Reduce 戦略を推奨。

#### ロールバック判定

| 指標 | 閾値 | 実測値 | 判定 |
|------|------|--------|------|
| TTFT P50 | > 3s | 0.31s - 1.13s | ✅ 正常 |
| Decode 速度 | < 30 tok/s | 52.1 - 57.5 tok/s | ✅ 正常 |
| OOM 発生率 | > 5% | 0% | ✅ 正常 |

**結論**: QAT モデル移行後のパフォーマンスは許容範囲内。ロールバック不要。

### PROS

1. **VRAM 使用量削減**: モデルサイズ約 21% 削減
2. **品質維持**: QAT により BF16 同等の品質を低ビット幅で実現
3. **ロールバック容易**: 環境変数で即時切り替え可能
4. **下位互換性**: 既存の API、パラメータに変更なし

### CONS, TRADEOFF

1. **初回ダウンロード**: 新しいモデルのダウンロードが必要（約 4GB）
2. **検証期間**: 品質の回帰テストが必要
3. **モデルキャッシュ**: 既存キャッシュのクリアが推奨

## APPENDIX

### デプロイ手順

```bash
# 1. コンテナ停止
docker compose -f compose/compose.yaml down news-creator-backend news-creator

# 2. モデルキャッシュクリア（推奨）
docker volume rm alt_news_creator_models || true

# 3. リビルド・起動
docker compose -f compose/compose.yaml build news-creator-backend news-creator
docker compose -f compose/compose.yaml up -d news-creator-backend news-creator

# 4. ログ確認
docker compose -f compose/compose.yaml logs news-creator-backend --tail=50
```

### ロールバック手順

```bash
# 環境変数でロールバック
export OLLAMA_BASE_MODEL=gemma3:4b
docker compose -f compose/compose.yaml down news-creator-backend
docker compose -f compose/compose.yaml up -d news-creator-backend
```

### ロールバックトリガー条件

- TTFT P50 > 3秒（50% 劣化）
- Decode 速度 < 30 tok/s
- OOM 発生率 > 5%
- 品質スコア（ROUGE）10% 以上低下

### 参考資料

- [Google Developers Blog - Gemma 3 QAT](https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/)
- [Ollama Models - gemma3](https://ollama.com/library/gemma3)
