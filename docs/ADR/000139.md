# pre-processor: タイムアウト繰り返し記事の永続スキップ機能 (Dead Letter Queue)

## ADR's STATUS

Accepted

## CONTEXT

### 背景

要約処理で特定の記事が繰り返しタイムアウトし、リトライを繰り返す悪循環が発生していた。

```
リトライ → タイムアウト → リトライ → タイムアウト ...
```

この状況により、news-creator のキュー待ち時間が増大し、他の正常な記事の処理にも影響を与えていた。

### 既存実装の分析

既に存在していたもの:
- `retry_count`, `max_retries` フィールド（DB とモデルに存在）
- `CanRetry()` メソッド（`retry_count < max_retries` をチェック）
- `UpdateJobStatus` で失敗時に `retry_count` をインクリメント

問題点:
1. `GetPendingJobs` が `status = 'pending'` のみ取得し、リトライ対象の `failed` ジョブを無視
2. 失敗後にリトライ可能なジョブを `pending` に戻す処理がない
3. `retry_count >= max_retries` のジョブを永続スキップする仕組みがない

## DECISION MAKING

### 検討したアプローチ

| Option | 内容 | 評価 |
|--------|------|------|
| A: Dead Letter Queue パターン | 既存の `retry_count`/`max_retries` を活用し、上限超過時に `dead_letter` ステータスへ移行 | 採用 |
| B: 別テーブルで管理 | 失敗記事を別テーブルに移動 | 不採用 |
| C: 削除 | 失敗記事を削除 | 不採用 |

### 採用理由

**Option A（Dead Letter Queue パターン）を採用**:
1. 既存の `retry_count`/`max_retries` フィールドを活用できる
2. 失敗記事の追跡が可能（削除と異なり履歴が残る）
3. 後から手動で再処理可能
4. スキーマ変更不要（ステータス値の追加のみ）

**Option B, C を不採用にした理由**:
- B: テーブル分割はクエリの複雑化を招く
- C: 削除すると障害分析や再処理ができなくなる

## RESULTS, EFFECTS

### 修正ファイル

| ファイル | 変更内容 |
|----------|----------|
| `models/summarize_job.go` | `SummarizeJobStatusDeadLetter` 定数追加、`IsTerminal()` 更新 |
| `repository/summarize_job_repository.go` | `UpdateJobStatus` でリトライ/dead_letter 振り分けロジック実装 |
| `service/summarize_queue_worker.go` | ログ改善（dead_letter 移行時の明示的なログ出力） |

### 主な変更

#### ステータス追加

```go
const (
    SummarizeJobStatusPending    SummarizeJobStatus = "pending"
    SummarizeJobStatusRunning    SummarizeJobStatus = "running"
    SummarizeJobStatusCompleted  SummarizeJobStatus = "completed"
    SummarizeJobStatusFailed     SummarizeJobStatus = "failed"
    SummarizeJobStatusDeadLetter SummarizeJobStatus = "dead_letter"  // 追加
)
```

#### UpdateJobStatus のロジック変更

失敗時の処理を以下のように変更:

```sql
UPDATE summarize_job_queue
SET
    status = CASE
        WHEN retry_count + 1 >= max_retries THEN 'dead_letter'
        ELSE 'pending'
    END,
    error_message = $1,
    completed_at = CASE
        WHEN retry_count + 1 >= max_retries THEN $2
        ELSE completed_at
    END,
    retry_count = retry_count + 1
WHERE job_id = $3
```

#### 状態遷移図

```
[pending] → [running] → [completed]
    ↑           |
    |           v
    +←←← [failed] (retry_count < max_retries)
                |
                v (retry_count >= max_retries)
          [dead_letter]
```

### 検証結果

- ビルド: パス
- ユニットテスト: 全件パス
- コンテナヘルスチェック: healthy

### PROS

1. **悪循環の解消**: タイムアウト記事が永続的に除外され、他の記事処理への影響を排除
2. **追跡可能性**: `dead_letter` ステータスで失敗記事を追跡可能
3. **再処理可能**: 手動で `status` を `pending` に戻せば再処理可能
4. **最小限の変更**: 既存フィールドを活用し、SQL ロジックのみ変更

### CONS, TRADEOFF

1. **自動復旧なし**: `dead_letter` に移行した記事は手動介入が必要
2. **通知機能なし**: 管理者への自動通知は未実装（今後の課題）

## APPENDIX

### デプロイ手順

```bash
# リビルド・再起動
docker compose -f compose/compose.yaml -p alt build pre-processor
docker compose -f compose/compose.yaml -p alt up -d pre-processor

# ヘルスチェック
curl http://localhost:9200/api/v1/health
```

### 検証方法

```sql
-- dead_letter に移行した記事を確認
SELECT article_id, status, retry_count, max_retries, error_message, created_at
FROM summarize_job_queue
WHERE status = 'dead_letter'
ORDER BY created_at DESC;

-- 手動で再処理する場合
UPDATE summarize_job_queue
SET status = 'pending', retry_count = 0
WHERE article_id = '<target_article_id>';
```

### ログ確認

```bash
# dead_letter 移行のログを確認
docker logs alt-pre-processor-1 --tail=100 | grep -i "dead_letter"
```

### 参考資料

- [Amazon SQS Dead Letter Queues Best Practices](https://www.ranthebuilder.cloud/post/amazon-sqs-dead-letter-queues-and-failures-handling-best-practices)
- [Dead Letter Queue: How to Handle Failed Messages](https://dev.to/mehmetakar/dead-letter-queue-3mj6)
