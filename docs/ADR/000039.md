# フィード取得・既読処理におけるデータ整合性の確保

## ステータス

**採択・実装完了（Accepted & Implemented）** - 2026年1月

## コンテキスト

### 背景

本番環境でフィード機能に2つの不具合が発生していた：

1. **フィード重複表示**: 同一フィードが複数回表示される
2. **既読処理の404エラー**: `MarkAsRead`が`article not found`を返す

### 問題1: フィード重複表示

`FetchUnreadFeedsListCursor`のSQLクエリがLEFT JOINで記事テーブルを結合していた：

```sql
SELECT f.id, ..., a.id AS article_id
FROM feeds f
LEFT JOIN articles a ON a.url = f.link AND a.deleted_at IS NULL
```

**発生条件**: 同じURLに複数の記事レコードが存在する場合（再取得やデータ重複）、1フィードに対して複数行が返される。

**HARファイル分析結果**:
- 全体の約30%のフィードで重複が発生

### 問題2: 既読処理の404エラー

`MarkArticleAsRead`はURL正規化を行ってからDBを検索：

```go
normalizedURL, _ := utils.NormalizeURL(articleURL.String())  // 末尾スラッシュ削除
getArticleQuery := `SELECT id FROM articles WHERE url = $1`
```

**発生条件**: DBに保存されているURLが正規化されていない（末尾スラッシュあり）場合、正規化後のURLでは一致しない。

**HARファイル分析結果**:
- 5件の`MarkAsRead`が404を返していた
- 例: DB `https://example.com/article/` vs リクエスト `https://example.com/article`

## 意思決定

### 決定1: フィード重複問題 - LEFT JOINからスカラーサブクエリへ

**採用したアプローチ**: JOINではなくスカラーサブクエリで`article_id`を取得

```sql
SELECT f.id, ...,
       (SELECT a.id FROM articles a WHERE a.url = f.link AND a.deleted_at IS NULL LIMIT 1) AS article_id
FROM feeds f
```

**代替案と理由**:
- ❌ **DISTINCT ON**: PostgreSQLでORDER BY制約があり、`created_at DESC`のソート順が崩れる
- ❌ **Gateway層での重複排除**: パフォーマンスとページネーションに影響
- ✅ **スカラーサブクエリ**: ORDER BY維持、1フィード=1行を保証

**パフォーマンス検証**:
```
旧（LEFT JOIN）:  1.35ms
新（サブクエリ）: 2.29ms
```
約1ms増加するが、実用上問題ないレベル。両方ともインデックス（`idx_articles_url`）を使用。

### 決定2: 既読処理の404問題 - ゼロトラスト正規化

**採用したアプローチ**: フロントエンド・バックエンドの両方でURL正規化を実施

**バックエンド**:
```go
// 正規化前後の両方でDB検索
getArticleQuery := `SELECT id FROM articles WHERE url = $1 OR url = $2 LIMIT 1`
err = r.pool.QueryRow(ctx, getArticleQuery, normalizedURL, originalURL).Scan(&articleID)
```

**フロントエンド**:
```typescript
function normalizeUrl(url: string): string {
    if (url.endsWith('/') && url !== '/') {
        return url.slice(0, -1);
    }
    return url;
}
```

**代替案と理由**:
- ❌ **フロントエンドのみで正規化**: バックエンドが信頼できないデータを受け取る
- ❌ **バックエンドのみで対応**: フロントエンドのバグが残る可能性
- ❌ **DBマイグレーション**: 既存データの修正が大規模で影響範囲が広い
- ✅ **ゼロトラスト（両側対応）**: 各レイヤーが自己防衛、将来の不整合も吸収

### 決定3: DBへのURL保存時の正規化は見送り

**理由**:
- 既存データとの整合性問題が発生
- pre-processorなど複数のデータ投入経路がある
- 今回のクエリ側対応で問題は解決

**将来検討事項**:
- 新規データ投入時の正規化は段階的に導入を検討
- URLカノニカライズの統一ルール策定

## 結果

### 技術的成果

| 項目 | 修正前 | 修正後 |
|------|--------|--------|
| フィード重複 | 約30%で発生 | 解消 |
| MarkAsRead 404 | 末尾スラッシュ不一致で発生 | 解消 |
| クエリ実行時間 | 1.35ms | 2.29ms (+0.94ms) |

### 修正ファイル

**バックエンド**:
- `alt-backend/app/driver/alt_db/fetch_feed_driver.go` - サブクエリ化
- `alt-backend/app/driver/alt_db/user_reading_status_driver.go` - OR検索追加

**フロントエンド**:
- `alt-frontend-sv/src/lib/connect/feeds.ts` - normalizeUrl関数追加

### PROS

1. **即時解決**: 本番の不具合を最小限の変更で修正
2. **後方互換性**: 既存データの変更不要
3. **防御的設計**: ゼロトラストにより将来の不整合も吸収
4. **パフォーマンス維持**: インデックスを活用、実用上影響なし

### CONS, TRADEOFF

1. **クエリ複雑化**: サブクエリとOR条件によりSQLが複雑に
2. **微小なオーバーヘッド**: 約1msの実行時間増加
3. **根本解決ではない**: DB内のデータ不整合は残存
4. **重複責務**: 正規化ロジックがフロントエンド・バックエンドに分散

## 付録

### 調査方法

1. **HARファイル分析**: ブラウザのネットワーク通信を記録し、リクエスト/レスポンスを解析
2. **DB直接クエリ**: 問題のURLがfeedsとarticlesにどう保存されているか確認
3. **EXPLAIN ANALYZE**: クエリプランの比較検証

### 関連ADR

- [ADR-038: REST API から Connect-RPC への完全移行](./000038.md) - APIの統一戦略

### 今後の改善

1. **URLカノニカライザの統一**: 全データ投入経路で`utils.NormalizeURL`を使用
2. **データクレンジング**: 既存データの正規化マイグレーション（優先度低）
3. **モニタリング**: 404エラーのダッシュボード化で早期検知
