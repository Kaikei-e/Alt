# Unified Ollama Model Configuration: COLD_START Prevention

## ADR's STATUS

Accepted

## CONTEXT

### 背景

LLM 生成速度が異常に遅い問題が発生 (0.32-6.33 tok/s、期待値 50-100 tok/s)。

調査の結果、**複数サービスが異なるモデル名を指定**しており、リクエストごとに Ollama がモデルをリロード (COLD_START) していたことが判明。

### 問題の構造

```
┌─────────────────┐     ┌─────────────────┐
│  news-creator   │     │  pre-processor  │
│ model=gemma3-   │     │ model=gemma3-   │
│     4b-8k       │     │     4b-12k      │
└────────┬────────┘     └────────┬────────┘
         │                       │
         ▼                       ▼
    ┌─────────────────────────────────┐
    │            Ollama               │
    │  COLD_START on every switch!    │
    │  load_duration: 7-10 seconds    │
    └─────────────────────────────────┘
```

### 影響

| 指標 | 異常値 | 期待値 |
|------|--------|--------|
| tokens/sec | 0.32-6.33 | 50-100 |
| load_duration | 7-10s | 0s |
| TTFT | 100s+ | < 2s |

### 根本原因

| サービス | 設定箇所 | 指定モデル |
|----------|----------|------------|
| news-creator | compose/ai.yaml | `gemma3-4b-8k` |
| pre-processor | quality_judger.go (ハードコード) | `gemma3-4b-12k` |
| recap-evaluator | config.py (デフォルト値) | `gemma3-4b-16k` |
| benchmark script | benchmark_real_articles.py | `gemma3-4b-12k` |

ADR 140 + ADR 145 の設計意図では、**全サービスが同一モデル (`gemma3-4b-8k`) を共有**して COLD_START を回避する想定だったが、各サービスのデフォルト値やハードコードが統一されていなかった。

## DECISION MAKING

### 検討したアプローチ

| Option | 内容 | 評価 |
|--------|------|------|
| A: デフォルト値統一 | 全サービスのデフォルトを `gemma3-4b-8k` に変更 | **採用** |
| B: 環境変数のみ | compose ファイルで環境変数を設定 | デフォルト値が残り、設定漏れリスク |
| C: 動的モデル選択 | リクエスト時にモデルを自動選択 | 複雑性増大、COLD_START 回避困難 |

**採用理由**:
- シンプルかつ確実
- 設定漏れのリスクを最小化
- ADR 145 の「8K × 2 スロット設計」に完全準拠

## RESULTS, EFFECTS

### 変更ファイル一覧

| ファイル | 変更内容 |
|----------|----------|
| `recap-evaluator/src/recap_evaluator/config.py` | `ollama_model` デフォルト: `gemma3-4b-16k` → `gemma3-4b-8k` |
| `pre-processor/app/quality-checker/quality_judger.go` | `modelName`: `gemma3-4b-12k` → `gemma3-4b-8k` |
| `pre-processor/app/quality-checker/quality_judger.go` | `NumCtx`: `12288` → `8192` |
| `pre-processor/app/quality-checker/quality_judger.go` | `maxQualityCheckContentLength`: `30,000` → `20,000` |
| `news-creator/app/scripts/benchmark_real_articles.py` | デフォルトモデル: `gemma3-4b-12k` → `gemma3-4b-8k` |
| `.env` | `OLLAMA_MODEL=gemma3-4b-8k` 追加 |

### 検証結果

```
# Before (モデル切り替え発生)
TTFT: 104.57s
load_duration: 9.73s
decode: 64.82 tok/s
tokens_per_second: 32.79

# After (統一後)
TTFT: 0.65s
load_duration: 0.2s
decode: 64.09 tok/s
tokens_per_second: 55.12
```

### Ollama モデル状態

```bash
$ curl -s localhost:11435/api/ps | jq '.models[].name'
"gemma3-4b-8k:latest"
# 単一モデルのみがロードされている
```

### PROS

1. **COLD_START 解消**: モデル切り替えによるリロードが発生しない
2. **TTFT 大幅改善**: 104.57s → 0.65s (160倍高速化)
3. **安定した生成速度**: 55-65 tok/s で期待値範囲内
4. **ADR 145 準拠**: 8K × 2 スロット設計の意図通りに動作

### CONS, TRADEOFF

1. **Context 縮小**: 12K/16K → 8K により、長文処理は Map-Reduce 必要
2. **pre-processor 制限**: Quality Checker の最大入力長が 30,000 → 20,000 文字に縮小

## APPENDIX

### デプロイ手順

```bash
# 1. 関連コンテナを再ビルド
docker compose -f compose/compose.yaml -p alt build \
  news-creator recap-evaluator pre-processor

# 2. 再起動
docker compose -f compose/compose.yaml -p alt up -d \
  news-creator recap-evaluator pre-processor

# 3. 古いモデルをアンロード (任意)
curl -s localhost:11435/api/generate -d '{"model":"gemma3-4b-12k","keep_alive":0}'
curl -s localhost:11435/api/generate -d '{"model":"gemma3-4b-16k","keep_alive":0}'
```

### 検証コマンド

```bash
# ロードされているモデル確認
curl -s localhost:11435/api/ps | jq '.models[] | {name, context_length}'
# 期待値: gemma3-4b-8k のみ

# COLD_START 監視
docker compose -f compose/compose.yaml -p alt logs -f news-creator | \
  grep -E "(COLD_START|load_duration)"

# 生成速度確認
docker compose -f compose/compose.yaml -p alt logs news-creator --since=1m | \
  grep "decode:"
# 期待値: 50-100 tok/s
```

### 関連 ADR

- **ADR 140**: Hybrid RT/BE Priority Semaphore
- **ADR 145**: 8K Context + OLLAMA_NUM_PARALLEL=2 設計

### 教訓

1. **デフォルト値の重要性**: 環境変数で上書きしても、デフォルト値が残っていると設定漏れの原因になる
2. **ハードコードの危険性**: 設定値は可能な限り環境変数や設定ファイルで管理すべき
3. **統合テストの必要性**: 複数サービスが同一リソースを共有する場合、統合テストで設定の整合性を確認すべき
