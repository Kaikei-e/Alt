# RAG用LLMをgpt-oss:20bからqwen3:14bへ移行

## ステータス

承認済み（2026-01-28）

## 背景

knowledge-augurサービスはRAGパイプラインにLLM機能を提供している。従来のモデル（`gpt-oss:20b`およびカスタム版`gpt-oss20b-igpu`）には以下の特性があった：

- JSON出力前の思考オーバーヘッドにより`num_predict=4096`が必要
- 推論モデルには温度0.7を推奨
- タスクの複雑さに応じたカスタム思考レベル（"low"、"medium"）を使用

性能向上とシンプルな設定を目指し、`qwen3:14b`を代替モデルとして評価した。

## 決定事項

### モデル選定基準

1. **思考オーバーヘッドなし**: qwen3はAPIを通じて明示的な思考モード制御をサポート
2. **パラメータ数削減**: 20Bから14Bへ、メモリフットプリントを削減
3. **推論高速化**: トークン/秒の改善が期待される
4. **ロールバック可能**: 環境変数によるモデル切り替え

### 実装アプローチ

1. **非破壊的な追加**: 既存モデルと並行して`qwen3-14b-rag`モデルを追加
2. **APIレベルでの思考制御**: Ollama APIリクエストで`think: false`（boolean）を使用
3. **モデル別パラメータ選択**: `OllamaGenerator.getThinkParam()`がモデル名に応じた値を返却
4. **環境変数設定**: `AUGUR_KNOWLEDGE_MODEL`で実行時にモデル選択

### 主要な設定の違い

| パラメータ | gpt-oss20b-igpu | qwen3-14b-rag |
|-----------|-----------------|---------------|
| `num_predict` | 4096 | 512 |
| `temperature` | 0.7 | 0.2 |
| `think`パラメータ | "low"/"medium" | false (boolean) |
| 停止トークン | `<\|end\|>`, `<\|eot_id\|>`, `<\|im_end\|>` | `<\|im_end\|>`, `<\|endoftext\|>` |

## 結果・影響

### 変更内容

| コンポーネント | ファイル | 変更内容 |
|-----------|------|--------|
| knowledge-augur | `Modelfile.qwen3-14b-rag` | qwen3固有パラメータを含む新規Modelfile |
| knowledge-augur | `Dockerfile` | 新規ModelfileのCOPYを追加 |
| knowledge-augur | `entrypoint.sh` | qwen3:14bのプル、カスタムモデル作成、環境変数ベースのプリロード |
| rag-orchestrator | `ollama_generator.go` | `Think`フィールドを`interface{}`に変更、`getThinkParam()`を追加 |
| rag-orchestrator | `config.go` | デフォルトモデルを`qwen3-14b-rag`に変更 |
| docs | `.env.template` | モデルコメントを更新 |
| docs | `knowledge-augur.md` | ドキュメントを更新 |

### メリット

- **出力トークン削減**: `num_predict`が4096から512へ（思考オーバーヘッドなし）
- **シンプルな出力**: `<think>`ブロックなしで直接コンテンツ生成
- **低い温度設定**: RAGユースケースに対してより決定論的な出力
- **小さいモデルサイズ**: パラメータ数が20Bから14Bへ
- **ロールバック対応**: `AUGUR_KNOWLEDGE_MODEL=gpt-oss20b-igpu`で元に戻せる

### デメリット・トレードオフ

- **Ollama API依存**: 思考制御には各リクエストで`think: false`が必要（Modelfileでは不可）
- **モデル固有のコードパス**: `getThinkParam()`にモデル名による条件分岐あり
- **初回プルのオーバーヘッド**: 初回起動時に約9GBのqwen3:14bモデルのダウンロードが必要

## 付録

### ロールバック手順

```bash
# 環境変数で簡単にロールバック
export AUGUR_KNOWLEDGE_MODEL=gpt-oss20b-igpu
docker compose -f compose/compose.yaml -p alt up -d rag-orchestrator
```

entrypoint.shが起動時に両モデルを作成するため、コンテナの再ビルドは不要。

### 動作確認コマンド

```bash
# 利用可能なモデルを確認
curl http://<augur-host>:11435/api/tags | jq '.models[].name'

# 生成テスト（思考無効）
curl -X POST http://<augur-host>:11435/api/chat \
  -H "Content-Type: application/json" \
  -d '{"model":"qwen3-14b-rag","messages":[{"role":"user","content":"Hello"}],"think":false,"stream":false}'
```

### 参考資料

- [Ollama Thinking Documentation](https://docs.ollama.com/capabilities/thinking)
- [Qwen3 Model Card](https://ollama.com/library/qwen3)
