# tag-generator 日本語タグ生成品質改善

## ADR's STATUS

Accepted

## CONTEXT

### 問題点

tag-generatorの日本語タグ抽出において、以下の品質課題が存在していた:

| 課題 | 影響 |
|------|------|
| セマンティック分析の欠如 | タグの文脈関連性が低い |
| 正規表現依存の複合語抽出 | 文法的に不正な候補が混入 |
| 名詞句境界の不正確さ | 複合名詞が分断される |

### 現状アーキテクチャ

**英語タグ抽出 (KeyBERT ベース)**
- モデル: `paraphrase-multilingual-MiniLM-L12-v2`
- 2段階抽出: 単語 (n-gram 1,1) + フレーズ (n-gram 2,3)
- MMR多様性アルゴリズム使用

**日本語タグ抽出 (Fugashi ベース)**
- トークナイザー: Fugashi + unidic-lite
- 頻度ベースランキングのみ
- 正規表現パターンによる複合語抽出
- **KeyBERT未使用**（セマンティック分析なし）

## DECISION MAKING

### 3フェーズ改善アプローチ

#### Phase 1: 基盤強化 + Fugashi改善

1. **評価フレームワーク構築**
   - ゴールドデータセット作成
   - メトリクス: Precision@K, Recall@K, F1, Diversity Score

2. **Fugashi複合名詞連結の改善**
   - 連続名詞トークンの自動連結ロジック
   - コネクタ（の、・）を含む複合名詞抽出

3. **ストップワード拡充**
   - 技術記事特有の汎用語追加

#### Phase 2: GiNZA導入

構文解析ベースの名詞句抽出でタグ品質を向上。

```
GiNZA (spaCy + Sudachi)
├─ 依存構文解析
├─ 名詞句チャンキング
└─ 固有表現認識 (NER)
```

#### Phase 3: KeyBERT日本語統合

```
候補生成 (Fugashi/GiNZA)
       ↓
KeyBERT セマンティックスコアリング
       ↓
頻度 × セマンティック 複合スコア
```

### ハイブリッド抽出戦略

```
入力テキスト
    ├─→ GiNZA (構文ベース)
    │     ├─ noun_chunks → 名詞句
    │     └─ ents → 固有表現
    │
    ├─→ Fugashi (頻度ベース)
    │     ├─ 連続名詞連結
    │     └─ 正規表現パターン
    │
    └─→ KeyBERT (セマンティック)
          └─ candidates パラメータでスコアリング

          ↓ 重み付け集約

    最終タグリスト (top_k)
```

## RESULTS, EFFECTS

### 新規ファイル

| ファイル | 内容 |
|---------|------|
| `tag_extractor/ginza_extractor.py` | GiNZA抽出器（名詞句/NER） |
| `tag_extractor/hybrid_extractor.py` | ハイブリッド抽出器 |
| `scripts/evaluate_tag_quality.py` | 評価フレームワーク |
| `evaluation/golden_dataset_ja_sample.json` | サンプルゴールドデータセット |

### 変更ファイル

| ファイル | 変更内容 |
|---------|---------|
| `tag_extractor/extract.py` | 複合名詞連結改善、KeyBERT日本語統合 |
| `tag_extractor/stopwords_ja.txt` | 技術記事ストップワード追加 |
| `tag_extractor/model_manager.py` | GiNZAモデル管理追加 |
| `pyproject.toml` | `ml-extended` 依存グループ追加 |

### 新規テスト

| テストファイル | テスト数 |
|---------------|---------|
| `test_ginza_extractor.py` | 18 |
| `test_hybrid_extractor.py` | 18 |
| `test_compound_noun_extraction.py` | 18 |

### 主要な実装

#### 1. 連続名詞連結 (`_extract_compound_nouns_fugashi`)

```python
def _extract_compound_nouns_fugashi(self, text: str) -> list[str]:
    parsed = list(self._ja_tagger(text))
    compounds = []
    current_compound = []

    for token in parsed:
        is_noun = token.feature.pos1.startswith("名詞")
        if is_noun:
            current_compound.append(token.surface)
        else:
            if len(current_compound) >= 2:
                compound = "".join(current_compound)
                if 3 <= len(compound) <= 30:
                    compounds.append(compound)
            current_compound = []
    return compounds
```

#### 2. KeyBERTセマンティックスコアリング

```python
def _score_japanese_candidates_with_keybert(
    self, text: str, candidates: list[str], freq_counter: Counter
) -> tuple[list[str], dict[str, float]]:
    # KeyBERT candidates パラメータで事前抽出候補をスコアリング
    keywords = self._keybert.extract_keywords(
        text,
        candidates=candidates,
        use_mmr=True,
        diversity=0.5,
    )
    # セマンティック 60% + 頻度 40% の複合スコア
    combined_score = (0.6 * semantic_score) + (0.4 * freq_score)
```

#### 3. ハイブリッド抽出器

```python
class HybridExtractor:
    def extract_tags(self, title: str, content: str) -> list[str]:
        # GiNZA候補（構文ベース）
        ginza_candidates = self._extract_candidates_ginza(text)

        # Fugashi候補（頻度ベース）
        fugashi_candidates = self._extract_candidates_fugashi(text)

        # KeyBERTスコアリング（オプション）
        self._score_with_keybert(text, all_candidates)

        # 重み付け集約・デデュプリケーション
        return sorted_by_combined_score[:top_k]
```

### 依存関係

```toml
# pyproject.toml
ml-extended = [
    "ginza>=5.2.0",
    "ja-ginza>=5.2.0",
    "spacy>=3.8.0",
    # + 既存 ml グループの全依存
]
```

### 設定オプション

| 設定 | デフォルト | 説明 |
|------|-----------|------|
| `use_japanese_semantic` | `True` | KeyBERTセマンティックスコアリング有効 |
| `japanese_mmr_diversity` | `0.5` | MMR多様性パラメータ |
| `use_ginza` | `True` | GiNZA使用（HybridExtractor） |
| `use_keybert_scoring` | `True` | KeyBERTスコアリング使用 |

### PROS

1. **品質向上**: 複合名詞の正確な抽出、セマンティック関連性
2. **柔軟性**: GiNZA/KeyBERT/Fugashi の組み合わせが設定可能
3. **後方互換**: 既存APIは維持、内部改善
4. **評価可能**: ゴールドデータセットによる定量評価

### CONS, TRADEOFF

1. **GiNZAモデルサイズ**: 約500MB追加（遅延ロードで緩和）
2. **推論レイテンシ**: GiNZA使用時は若干増加
3. **依存関係**: spaCy + GiNZA の追加インストールが必要

## APPENDIX

### 使用方法

```bash
# 標準インストール（GiNZAなし）
uv sync --group ml

# 拡張インストール（GiNZA込み）
uv sync --group ml-extended

# 評価実行
uv run python scripts/evaluate_tag_quality.py \
  --dataset evaluation/golden_dataset_ja_sample.json \
  --extractor current

# テスト実行
cd tag-generator/app && uv run pytest tests/ -v
```

### 成功基準（目標値）

| 指標 | 現状（推定） | 目標 |
|------|-------------|------|
| 日本語 Precision@5 | 0.50 | 0.70 |
| 日本語 Recall@10 | 0.40 | 0.60 |
| 推論レイテンシ (p95) | 180ms | < 300ms |
| 空タグ率 | 3% | < 2% |

### 関連ADR

- ADR-168: オンザフライタグ生成
- ADR-173: StreamArticleTags実装
