# LLM Modelfile Optimization: Preventing Excessive Token Generation

## ADR's STATUS

Accepted

## CONTEXT

### 背景

RAG パイプラインで使用している 20B パラメータの LLM において、短いクエリに対して過剰なトークン生成が発生し、レスポンス時間が著しく悪化していた。

### 問題の構造

```
┌─────────────────────────────────────────────────────┐
│                  Short Query                         │
│              "What is RSS?"                          │
└────────────────────┬────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────┐
│                    LLM                               │
│         num_predict: unlimited (default)             │
│         stop tokens: none configured                 │
└────────────────────┬────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────┐
│              Generated Output                        │
│         2,558 tokens (expected: ~200)                │
│         Total time: 130 seconds                      │
└─────────────────────────────────────────────────────┘
```

### ベンチマーク結果 (最適化前)

| クエリタイプ | TTFT (Mean) | Total Duration (Mean) | 生成トークン数 |
|-------------|-------------|----------------------|---------------|
| Short (~50 tokens) | 12.07s | 86.24s | 1,220-2,558 |
| Medium (~200 tokens) | 2.97s | 12.38s | 220-291 |
| Long (~500 tokens) | 6.32s | 31.12s | 523-726 |

### 根本原因

1. **`num_predict` 未設定**: 出力トークン数の上限がなく、モデルが自発的に停止するまで生成を継続
2. **Stop tokens 未設定**: EOS トークンが設定されておらず、適切な終了ポイントが不明確
3. **iGPU 環境への最適化不足**: CPU 向け設定のまま使用していた

## DECISION MAKING

### 検討したアプローチ

| Option | 内容 | 評価 |
|--------|------|------|
| A: num_predict 制限 | 出力トークン数を 512 に制限 | **採用** |
| B: 量子化モデル (Q4_K_M) | メモリ効率化で decode 速度向上 | 将来検討 |
| C: プロンプト改善 | 「簡潔に回答せよ」等の指示追加 | 不確実性が高い |

**Option A を採用した理由**:
- 確実に過剰生成を防止できる
- 既存の品質に影響を与えない (RAG 用途では 512 tokens で十分)
- 実装が容易で即座に効果を確認可能

### 新規 Modelfile の設計

```dockerfile
FROM base-model:20b

# Context window - 8K tokens for RAG workloads
PARAMETER num_ctx 8192

# Output limit - prevents runaway generation
PARAMETER num_predict 512

# Low temperature for consistent responses
PARAMETER temperature 0.2

# Batch size for iGPU optimization
PARAMETER num_batch 512

# Stop tokens for proper termination
PARAMETER stop "<|end|>"
PARAMETER stop "<|eot_id|>"
PARAMETER stop "<|im_end|>"
```

## RESULTS, EFFECTS

### 変更ファイル一覧

| ファイル | 変更内容 |
|----------|----------|
| `knowledge-augur/Modelfile.gpt-oss20b-igpu` | 新規作成 (iGPU 最適化設定) |
| `knowledge-augur/Dockerfile` | 新 Modelfile のコピー追加 |
| `knowledge-augur/entrypoint.sh` | iGPU モデルの作成・プリロード |
| `rag-orchestrator/internal/infra/config/config.go` | デフォルトモデルを igpu 版に変更 |
| `docs/services/knowledge-augur.md` | ドキュメント更新 |

### ベンチマーク結果 (最適化後)

| クエリタイプ | Before | After | 改善率 |
|-------------|--------|-------|--------|
| Short Total | 86.24s | 16.35s | **81% 改善** |
| Medium Total | 12.38s | 3.46s | 72% 改善 |
| Long Total | 31.12s | 4.33s | 86% 改善 |
| Decode Speed | ~20 tok/s | ~20 tok/s | 変化なし (帯域制限) |

### PROS

1. **過剰生成の防止**: 512 トークン上限により、Short クエリの暴走を防止
2. **レスポンス時間の大幅改善**: 最大 130s → 25s 以下に短縮
3. **Stop tokens による適切な終了**: 複数の EOS トークンに対応
4. **後方互換性維持**: 既存の CPU モデルも保持 (legacy fallback)

### CONS, TRADEOFF

1. **出力の切り詰め可能性**: 512 トークンを超える回答は途中で切れる
   - RAG 用途では 512 tokens で十分と判断
   - 長文が必要な場合は呼び出し側で `num_predict` を上書き可能

2. **Decode 速度は変わらず**: iGPU のメモリ帯域制限により ~20 tok/s が上限
   - 更なる改善には Q4_K_M 量子化が必要 (Phase 3 として将来検討)

## APPENDIX

### デプロイ手順

```bash
# 1. 呼び出し元 (rag-orchestrator) をリビルド
docker compose -f compose/compose.yaml -p alt build rag-orchestrator

# 2. 再起動
docker compose -f compose/compose.yaml -p alt up -d rag-orchestrator

# 3. リモートの Ollama サーバーで新モデル作成 (必要な場合)
ollama create gpt-oss20b-igpu -f Modelfile.gpt-oss20b-igpu
```

### 検証コマンド

```bash
# モデル設定確認
curl -s http://localhost:11435/api/show -d '{"name":"gpt-oss20b-igpu"}' | \
  jq '.parameters'
# 期待値: num_predict=512, stop tokens 設定済み

# ベンチマーク実行
python knowledge-augur/scripts/benchmark_augur.py \
  --model gpt-oss20b-igpu \
  --iterations 5
```

### 今後の検討事項 (Phase 3)

- **Q4_K_M 量子化**: メモリ使用量 75% 削減、decode 速度 20 → 25-30 tok/s 期待
- **KV Cache 量子化**: `OLLAMA_KV_CACHE_TYPE=q4_0` でさらなるメモリ効率化

### 関連 ADR

- **ADR 019**: LLM 推論パフォーマンス設計
- **ADR 025**: GPU オフロード最適化
