# news-creator からの Ollama 分離

## ステータス

採択（Accepted）

## コンテキスト

### 背景

news-creator サービスは FastAPI アプリケーションと Ollama LLM サーバーを単一コンテナで実行していた。ベースイメージとして `ollama/ollama:latest` を使用し、その上に Python 環境と依存関係をインストールする構成だった。

### 問題点

| 問題 | 影響 |
|------|------|
| イメージ肥大化 | ~5GB（Ollama + Python + sentence-transformers） |
| デプロイ速度低下 | アプリコード変更でも全体を再ビルド（~3分） |
| 関心の分離不足 | GPU/モデル管理とアプリケーションロジックが混在 |
| テスト困難性 | Ollama 起動なしでのユニットテストが煩雑 |

### 既存の良い設計

既に Clean Architecture パターンが適用されており、`LLMProviderPort` 抽象インターフェースを通じて Ollama と通信する設計だった。この設計により、分離は比較的容易だった。

## 決定

### 2サービス構成への分離

```
┌─────────────────────────┐     HTTP      ┌─────────────────────────┐
│   news-creator          │──────────────▶│  news-creator-backend   │
│   (FastAPI App)         │   :11435      │  (Ollama Server)        │
│   Port: 11434           │               │  Port: 11435            │
│   Image: python:3.11-slim│              │  Image: ollama/ollama   │
└─────────────────────────┘               └─────────────────────────┘
```

### news-creator-backend（Ollama サービス）

- ベースイメージ: `ollama/ollama:latest`
- 責務: LLM 推論、モデル管理、GPU リソース管理
- Modelfile による 16K/60K コンテキストウィンドウモデルの作成
- GPU 使用確認と自動フェイルファスト

### news-creator（FastAPI アプリ）

- ベースイメージ: `python:3.11-slim`
- 責務: API エンドポイント、ビジネスロジック、Re-ranking
- `LLM_SERVICE_URL` 環境変数で backend に接続
- sentence-transformers（Cross-encoder）は引き続き含む

## 結果

### 変更ファイル

#### 新規作成

| ファイル | 説明 |
|---------|------|
| `news-creator/Dockerfile.backend` | Ollama 専用 Dockerfile |
| `news-creator/entrypoint-backend.sh` | Ollama 起動スクリプト |
| `news-creator/Dockerfile.app` | FastAPI アプリ用軽量 Dockerfile |
| `news-creator/app/tests/conftest.py` | pytest fixtures（TDD 改善） |
| `news-creator/app/tests/fakes/fake_llm_provider.py` | FakeProvider 実装 |

#### 修正

| ファイル | 変更内容 |
|---------|---------|
| `compose/ai.yaml` | 2サービス構成に分離 |
| `news-creator/app/pyproject.toml` | httpx 追加（TestClient 用） |

#### 削除

| ファイル | 理由 |
|---------|------|
| `news-creator/Dockerfile.creator` | 新 Dockerfile で置き換え |
| `news-creator/entrypoint.sh` | 分離した entrypoint で置き換え |

### Docker Compose 構成

```yaml
services:
  news-creator-backend:
    build:
      dockerfile: Dockerfile.backend
    ports:
      - "11435:11435"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11435/api/tags"]
      start_period: 120s  # モデルロードに時間がかかる

  news-creator:
    build:
      dockerfile: Dockerfile.app
    environment:
      - LLM_SERVICE_URL=http://news-creator-backend:11435
    depends_on:
      news-creator-backend:
        condition: service_healthy
    ports:
      - "11434:11434"
    healthcheck:
      start_period: 30s  # 軽量なので起動が速い
```

### TDD 改善

#### conftest.py の主要 fixture

```python
@pytest.fixture
def mock_llm_provider() -> AsyncMock:
    """LLMProviderPort のモック。Ollama 不要でテスト可能。"""
    provider = AsyncMock(spec=LLMProviderPort)
    provider.generate.return_value = LLMGenerateResponse(...)
    return provider

@pytest.fixture
def test_app(mock_llm_provider):
    """依存関係をモックした FastAPI アプリ。"""
    container.ollama_gateway = mock_llm_provider
    yield app
```

#### FakeLLMProvider

Integration テスト用の Fake 実装。モックより高機能で、呼び出し履歴追跡やパターンマッチングによるレスポンス設定が可能。

```python
fake_provider = FakeLLMProvider()
fake_provider.add_json_response("summarize", title="Test", bullets=["Point 1"])
```

### 期待される効果

| 指標 | Before | After |
|------|--------|-------|
| news-creator イメージサイズ | ~5GB | ~1.5GB |
| アプリ変更時のビルド時間 | ~3分 | ~30秒 |
| Ollama モデル更新時のアプリ再ビルド | 必要 | 不要 |
| ユニットテスト実行 | Ollama 起動必要 | 不要 |

### 変更不要だったファイル

| ファイル | 理由 |
|---------|------|
| `llm_provider_port.py` | 既に抽象化済み |
| `ollama_gateway.py` | HTTP 通信で設計済み |
| `config.py` | 環境変数から読み込み済み |

## 実装日

2026-01-14

## 関連

- [FastAPI Testing](https://fastapi.tiangolo.com/tutorial/testing/)
- [FastAPI Dependency Injection Best Practices](https://pytutorial.com/fastapi-dependency-injection-best-practices/)
- [Testing FastAPI Dependency Injection](https://hrekov.com/blog/testing-fastapi-dependency-injection)
