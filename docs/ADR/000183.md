# tag-generator 日本語KeyBERT Analyzer修正

## ADR's STATUS

Accepted (実装完了)

## CONTEXT

### 問題点

`extract.py` の `TagExtractor` が日本語タグ抽出時に文章断片を返す問題が発生。

```python
# 実際の出力例 (修正前)
Tags: ['今日はDatabricksのセキュリティについて学びます',
       'Databricksのセキュリティは',
       'データセキュリティとコンプライアンス', ...]
```

一方、`HybridExtractor` は正常に動作していた。

```python
# HybridExtractor の出力
Tags: ['Databricks', 'セキュリティ', 'UnityCatalog', '基盤づくり', ...]
```

### 根本原因

`extract.py:664-677` の `CountVectorizer` で `token_pattern=r"(?u)\b\w+\b"` を使用していたが、日本語にはスペースがないため `\b`（単語境界）が機能しない。

```python
# 問題のコード
vectorizer = CountVectorizer(
    lowercase=False,
    token_pattern=r"(?u)\b\w+\b",  # 日本語で機能しない
)
```

結果として:
1. CountVectorizer が candidates を正しくトークン化できない
2. KeyBERT がフォールバックでテキストから直接 n-gram を抽出
3. 文章断片がそのままタグとして返される

### 参考資料

- [scikit-learn CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)
- [KeyBERT Candidate Keywords](https://maartengr.github.io/KeyBERT/guides/countvectorizer.html)

## DECISION MAKING

### 解決策

Fugashi を使った日本語対応 analyzer を作成し、`token_pattern` の代わりに使用。

```python
def _make_japanese_analyzer(self):
    """Fugashi を使った日本語対応 analyzer を返す"""
    tagger = self._ja_tagger

    def analyzer(text: str) -> list[str]:
        if tagger is None:
            return text.split()

        tokens = []
        for word in tagger(text):
            # 名詞のみ抽出（candidates とマッチさせるため）
            if word.feature.pos1.startswith("名詞"):
                tokens.append(word.surface)
            # 英単語も保持
            elif word.surface.isascii() and word.surface.isalpha():
                tokens.append(word.surface)
        return tokens

    return analyzer
```

KeyBERT 呼び出し時に `analyzer` パラメータを使用:

```python
vectorizer = CountVectorizer(
    analyzer=self._make_japanese_analyzer(),
    lowercase=False,
)
```

### 技術的詳細

| 項目 | 修正前 | 修正後 |
|------|--------|--------|
| トークン化方式 | `token_pattern` (正規表現) | `analyzer` (Fugashi) |
| 日本語対応 | 不完全（単語境界なし） | 完全（形態素解析） |
| 英語対応 | 正常 | 正常（ASCII保持） |
| フォールバック | なし | `text.split()` |

## RESULTS, EFFECTS

### 変更ファイル

| ファイル | 変更内容 |
|---------|---------|
| `tag_extractor/extract.py` | `_make_japanese_analyzer()` メソッド追加、`_score_japanese_candidates_with_keybert()` で使用 |
| `tests/unit/test_keybert_japanese_scoring.py` | `TestMakeJapaneseAnalyzer` クラス追加 (6テスト) |
| `tests/unit/test_hybrid_extractor.py` | `MockCountVectorizer` に `analyzer` パラメータ追加 |

### 新規テスト

| テストクラス | テスト数 | 内容 |
|------------|---------|------|
| `TestMakeJapaneseAnalyzer` | 6 | analyzer存在確認、callable確認、名詞抽出、英語保持、フォールバック、vectorizer適用 |

### 修正後の出力例

```python
# 修正後
Tags: ['USB', 'PC', 'ケーブル', 'Android', 'iPhone', 'Ansible', ...]
```

### PROS

1. **正確なトークン化**: Fugashi による形態素解析で正確な単語境界を検出
2. **candidates とのマッチング向上**: 名詞ベースのトークン化で候補との一致率向上
3. **混合テキスト対応**: 日本語・英語混合テキストでも正常動作
4. **フォールバック機能**: tagger 未初期化時も動作継続

### CONS, TRADEOFF

1. **Fugashi依存**: tagger が必須（フォールバックあり）
2. **名詞限定**: 動詞・形容詞はトークンとして抽出されない（意図的な設計）

## APPENDIX

### TDD実装フロー

```
1. テスト作成 (RED)
   └─ test_keybert_japanese_scoring.py: TestMakeJapaneseAnalyzer 追加
   └─ 全テスト: FAILED (AttributeError: '_make_japanese_analyzer')

2. 実装 (GREEN)
   └─ extract.py: _make_japanese_analyzer() 追加
   └─ _score_japanese_candidates_with_keybert(): analyzer 使用に変更
   └─ 全テスト: PASSED (360 tests)

3. リファクタリング (REFACTOR)
   └─ ruff format 適用
   └─ MockCountVectorizer に analyzer パラメータ追加
```

### analyzer の動作フロー

```
入力テキスト: "GitHubでコードを管理"
    ↓
Fugashi形態素解析
    ↓
[GitHub(名詞), で(助詞), コード(名詞), を(助詞), 管理(名詞)]
    ↓
名詞・英語フィルタ
    ↓
出力トークン: ["GitHub", "コード", "管理"]
```

### 関連ADR

- ADR-176: tag-generator 日本語タグ生成品質改善
- ADR-181: tag-generator KeyBERT日本語スコアリング修正（lowercase=False）
- ADR-182: tag-generator 日本語タグ品質フィルタリング
