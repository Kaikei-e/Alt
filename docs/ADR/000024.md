# RAG推論レベル動的選択による応答速度改善

## ステータス

採択（Accepted） - 2025年12月30日
更新 - 2025年12月30日（根本原因特定と修正）

## コンテキスト

2025年12月下旬、ADR 000023のプロンプト最適化により、rag-orchestratorサービスのRAG回答品質は大幅に向上した。しかし、新たに重大なパフォーマンス問題が発覚した。

### 問題の詳細

**事象:**
ADR 000023の実装後、回答品質は「非常に良くなった」（ユーザーフィードバック）が、システムが正常に動作しなくなった。

**症状:**
- クエリ拡張が失敗（`response_length: 0`）
- Ollamaから空のレスポンス
- フォールバックが頻発

**ユーザー要求:**
- 現在のハードウェア（CPU推論のみ、GPU不可）で実現可能な最適化
- システムの動作修復

### 環境制約

- **推論環境**: CPU推論のみ（Ollama経由でgpt-oss20b-cpu）
- **Reasoning Level**: `medium`（ADR 000023で"low"から変更）
- **既存の最適化**: Ollama設定は`compose.augur.yaml`とDockerfileで実装済み
- **ストリーミング**: 既に実装済み（`POST /v1/rag/answer/stream`エンドポイント）だがフロントエンド未使用

### 根本原因の特定

ログ分析により、以下の根本原因を特定：

1. **ADR 000023の副作用**
   - すべてのLLM呼び出しで`Think: "medium"`を使用するように変更
   - クエリ拡張は`maxTokens: 200`で実行
   - `Think: "medium"`は思考プロセスにトークンを使用
   - 結果：200トークン予算が思考だけで消費され、実際の出力が0になる

2. **ユーザーの重要な洞察**
   - 「maxToken200の理由は？」という質問が問題発見のきっかけ
   - クエリ拡張の短いタスクに`Think: "medium"`は過剰

3. **誤った初期診断**
   - 当初、スコア閾値0.6のフィルタリングを実装
   - しかし「数値に根拠がない」（ユーザー指摘）
   - 実際の問題はコンテキストフィルタリングではなく、LLM推論設定だった

## 決定事項

### 採用した解決策: 動的Reasoning Level選択

**実装内容:**

タスクの複雑度（maxTokensから推測）に基づいて、Think levelを動的に選択：

```go
// internal/adapter/rag_augur/ollama_generator.go

// Determine Think level based on task complexity
// Short tasks (maxTokens < 300) use "low" - e.g., query expansion
// Longer tasks use "medium" - e.g., knowledge synthesis
think := "medium"
if maxTokens > 0 && maxTokens < 300 {
    think = "low"
}
```

**適用箇所:**
- `Generate()` メソッド（lines 106-139）
- `GenerateStream()` メソッド（lines 229-254）

**ロジック:**
- `maxTokens < 300`: `Think: "low"` → クエリ拡張などの短いタスク
- `maxTokens >= 300`: `Think: "medium"` → RAG回答生成などの複雑なタスク

**理由:**
- クエリ拡張は単純なタスク（5つの類似クエリ生成）で`Think: "low"`で十分
- RAG回答生成は複雑なタスク（情報統合）で`Think: "medium"`が必要
- トークン予算に基づく判断は、タスク複雑度の良い近似

### 削除した実装: スコア閾値フィルタリング

**当初実装した内容:**
```go
// 削除されたコード
const minScoreThreshold = 0.6
filtered := make([]ContextItem, 0, len(contexts))
for _, ctx := range contexts {
    if ctx.Score >= minScoreThreshold {
        filtered = append(filtered, ctx)
        if len(filtered) >= maxChunks {
            break
        }
    }
}
contexts = filtered
```

**削除理由:**
- 「数値に根拠がない」（ユーザー指摘）
- 実際の問題はコンテキスト品質ではなく、LLM推論設定だった
- 有用なコンテキストも除外してしまうリスク

**最終実装:**
```go
// 現在のコード（シンプル）
contexts := retrieved.Contexts

// Limit to maxChunks
if len(contexts) > maxChunks {
    contexts = contexts[:maxChunks]
}

result.contexts = contexts
```

## 結果

### 実装ファイル

| ファイル | 変更箇所 | 変更内容 |
|---------|---------|---------|
| `internal/adapter/rag_augur/ollama_generator.go` | Lines 106-139, 229-254 | 動的Think level選択を追加 |
| `internal/infra/config/config.go` | Line 42 | デフォルトmax chunks: `10` → `7`（保持） |
| `internal/usecase/answer_with_rag_usecase.go` | Lines 323-335 | スコア閾値削除、シンプルなmaxChunks制限に戻す |
| `internal/usecase/answer_with_rag_usecase_test.go` | Lines 77, 125 | テストmaxChunks: `10` → `7`（保持） |
| `internal/usecase/answer_with_rag_usecase_test.go` | Lines 163-211 | スコアフィルタリングテスト削除 |

### テスト結果

**すべてのRAGテスト合格:**
```
✅ TestAnswerWithRAG_Success
✅ TestAnswerWithRAG_Fallback
```

### 実測パフォーマンス

**修正前:**
- クエリ拡張: `response_length: 0`（失敗）
- システム状態: 動作不良（フォールバック頻発）

**修正後:**
- クエリ拡張: `response_length: 393`（成功）
- 拡張クエリ生成: 5つの英語クエリを正常生成
- コンテキスト取得: 210ヒットから10コンテキストを取得
- 総時間: 約44秒（元の40-60秒範囲内）
- システム状態: 正常動作

**実測ログ例:**
```json
{
  "msg": "ollama_generate_started",
  "max_tokens": 200,
  "prompt_length": 560
}
{
  "msg": "ollama_generate_completed",
  "response_length": 393  // ← 修正前は0
}
{
  "msg": "query_expanded",
  "expanded": [
    "What is an NPU and how does it work in modern AI hardware?",
    "NPU vs GPU vs TPU: differences, advantages, and use cases in 2025",
    "Deep dive into Neural Processing Units: architecture, performance benchmarks, and industry adoption",
    "How to program an NPU: SDKs, frameworks, and best practices for developers",
    "Recent breakthroughs in NPU technology: 2025 trends, chip designs, and market outlook"
  ]
}
```

## 影響

### ポジティブな影響

1. **システムの正常化**
   - クエリ拡張が正常動作
   - フォールバック頻度が適正レベルに
   - LLMが適切に応答を生成

2. **タスク別最適化**
   - 短いタスク（クエリ拡張）: 高速実行
   - 長いタスク（RAG回答）: 高品質維持

3. **保守性向上**
   - 複雑なスコアフィルタリングロジック削除
   - シンプルで理解しやすいコード
   - 根拠のない閾値を排除

4. **柔軟性**
   - maxTokensで自動的にThink levelが調整される
   - 将来的なタスク追加にも対応可能

### ネガティブな影響（最小限）

1. **閾値の妥当性**
   - 300トークンが短い/長いタスクの境界として適切か
   - **対応:** 実運用で監視、必要に応じて調整

2. **エッジケース**
   - 300トークン前後のタスクで最適でない可能性
   - **緩和策:** 現在のユースケース（200トークンと512+トークン）では問題なし

### 運用への影響

1. **環境変数の変更**
   - `RAG_DEFAULT_MAX_CHUNKS`: 10

2. **モニタリング項目**
   - クエリ拡張の成功率（`response_length > 0`）
   - Fallback率（適正レベルを維持）
   - 平均レスポンス時間

3. **フロントエンド統合（未完了）**
   - ストリーミングエンドポイントへの切り替え
   - SSEイベントハンドリング実装（Phase 1）

## 学んだ教訓

1. **症状と原因の区別**
   - 初期診断でスコアフィルタリングを疑った
   - ユーザーの「maxToken200の理由は？」という質問が根本原因発見のきっかけ
   - 根拠のない数値（0.6閾値）は避けるべき

2. **ADRの副作用**
   - ADR 000023の`Think: "medium"`変更がクエリ拡張に影響
   - グローバル変更は全ユースケースへの影響を考慮すべき

3. **ユーザーフィードバックの重要性**
   - ユーザーの疑問が問題解決の鍵になる
   - 根拠のない実装を指摘されたことで、より良い解決策に到達

## 参考文献

### Ollama Think Parameter
- Ollama公式ドキュメント: Thinkパラメータはモデルの推論深度を制御
- "low": 基本的なタスク向け、トークン消費少ない
- "medium": 複雑な推論タスク向け、思考プロセスにトークン使用

### GPT-OSS最適化
- [GPT-OSS Context Budget Guide](https://cookbook.openai.com/articles/gpt-oss-safeguard-guide): 最適コンテキストサイズ400-600トークン

## メタデータ

- **作成日:** 2025年12月30日
- **更新日:** 2025年12月30日
- **関連ADR:**
  - ADR 000023: RAGプロンプト最適化（本ADRの発端）
  - ADR 000022: クエリ拡張の導入（影響を受けたコンポーネント）
- **影響を受けるコンポーネント:**
  - rag-orchestrator
- **レビュー状態:** 実装完了、テスト合格、本番検証完了
- **次のアクション:**
  1. フロントエンドのストリーミング統合（Phase 1）
  2. 本番環境でのクエリ拡張成功率監視
  3. Think level閾値（300トークン）の妥当性検証
