# news-creator: 8K Context + OLLAMA_NUM_PARALLEL=2 による並行処理復活

## ADR's STATUS

Accepted

## CONTEXT

### 背景

news-creator サービスで `OLLAMA_NUM_PARALLEL=1` 設定により Head-of-Line Blocking が発生していた。

| 問題 | 影響 |
|------|------|
| RT (Real-Time/Streaming) リクエストがブロック | BE (Batch) 処理完了まで待機 |
| HybridPrioritySemaphore の効果なし | RT 専用スロットが活用されない |
| TTFT (Time To First Token) 悪化 | ストリーミング体験の低下 |

### 制約条件

- GPU: 8GB VRAM
- 使用モデル: Gemma3 4B QAT (Q4_0 量子化)

### VRAM 試算

| 構成 | 合計 Context | KV Cache | 総 VRAM | ステータス |
|------|-------------|----------|--------|-----------|
| 2 x 12K | 24K tokens | ~4.3 GB | ~8.5 GB | OOM |
| 1 x 12K | 12K tokens | ~2.1 GB | ~6.3 GB | 変更前 |
| **2 x 8K** | **16K tokens** | **~2.9 GB** | **~7.1 GB** | **採用** |

### 8K カバレッジ分析

実際の記事データ (約 20,000 件) を分析:

| Percentile | トークン数 | 8K Context 対応 |
|------------|-----------|-----------------|
| P50 | ~2,700 | OK |
| P75 | ~4,500 | OK |
| P90 | ~8,000 | 境界 |
| P95 | ~12,700 | Map-Reduce 必要 |

**適合率**: 約 89% の記事が 8K 以内に収まる。残り約 11% は Map-Reduce 処理で対応。

## DECISION MAKING

### 検討したアプローチ

| Option | 内容 | 評価 |
|--------|------|------|
| A: 8K + Parallel=2 | Context を 8K に削減、並行処理を復活 | **採用** |
| B: 12K + Parallel=1 維持 | Head-of-Line Blocking を許容 | 不採用 |
| C: GPU アップグレード | より大きな VRAM の GPU へ | コスト面で不採用 |
| D: KV Cache 量子化 | 実験的機能で安定性懸念 | 不採用 |

**採用理由**:
- 89% の記事は直接処理可能
- 残り 11% は既存の Map-Reduce 機能で対応済み
- VRAM 7.1GB で 8GB 以内に収まる

## RESULTS, EFFECTS

### 変更ファイル一覧

#### 設定ファイル

| ファイル | 変更内容 |
|----------|----------|
| `compose/ai.yaml` | `LLM_MODEL=gemma3-4b-8k`, `LLM_KEEP_ALIVE_8K` |
| `.env` | `LLM_NUM_CTX=8192` (既に設定済み) |

#### Ollama バックエンド

| ファイル | 変更内容 |
|----------|----------|
| `news-creator/Modelfile.gemma3-4b-8k` | `num_ctx 8192` で作成 |
| `news-creator/Dockerfile.backend` | 8K Modelfile をコピー |
| `news-creator/entrypoint-backend.sh` | 8K モデルの作成・プリロード |

#### アプリケーションコード

| ファイル | 変更内容 |
|----------|----------|
| `config.py` | デフォルト値を 8K 対応に変更 |
| `model_router.py` | `BUCKET_12K` → `BUCKET_8K` (8192) |
| `model_warmup.py` | 8K モデルのウォームアップ |
| `expand_query_usecase.py` | `EXPANSION_MODEL=gemma3-4b-8k` |

### 設定パラメータ変更

| パラメータ | 変更前 | 変更後 |
|-----------|--------|--------|
| `LLM_NUM_CTX` | 12288 | 8192 |
| `BUCKET` | BUCKET_12K (12288) | BUCKET_8K (8192) |
| `hierarchical_threshold_chars` | 12,000 | 8,000 |
| `hierarchical_chunk_max_chars` | 10,000 | 6,000 |
| `hierarchical_single_article_threshold` | 25,000 | 20,000 |
| `hierarchical_single_article_chunk_size` | 10,000 | 6,000 |
| `recursive_reduce_max_chars` | 10,000 | 6,000 |

### 検証結果

```
# VRAM 使用量 (実測)
memory.used: 5547 MiB / 8188 MiB (67.7%)

# Ollama ログ確認
- "8K model preloaded successfully"
- "Parallel:2"
- "FlashAttention:Enabled"
- "KvSize:16384" (= 2 slots x 8192)
- "offloaded 35/35 layers to GPU"
```

### PROS

1. **Head-of-Line Blocking 解消**: RT リクエストが即座に処理開始
2. **TTFT 改善**: ストリーミング応答の遅延解消
3. **VRAM 余裕**: 5.5GB / 8GB で安定動作
4. **スループット向上**: 並行処理により全体処理量が向上
5. **既存機能活用**: Map-Reduce は既に実装済み、変更不要

### CONS, TRADEOFF

1. **大きな記事の追加処理**: 約 11% の記事で Map-Reduce が必要
2. **Map-Reduce オーバーヘッド**: 長文記事では複数回の LLM 呼び出し
3. **品質への影響**: 極端に長い記事では要約精度が若干低下する可能性

## APPENDIX

### デプロイ手順

```bash
# 1. コンテナ再ビルド
docker compose -f compose/compose.yaml -p alt build news-creator-backend news-creator

# 2. 再起動
docker compose -f compose/compose.yaml -p alt up -d news-creator-backend news-creator
```

### 検証コマンド

```bash
# VRAM 監視
nvidia-smi -l 1

# ログ確認 (8K モデル、Parallel=2)
docker compose -f compose/compose.yaml -p alt logs news-creator-backend -f | grep -E "8K|Parallel|KvSize"

# Map-Reduce 動作確認
docker compose -f compose/compose.yaml -p alt logs news-creator -f | grep "Map phase"
```

### ロールバック手順

問題発生時:

```bash
# 1. 設定を戻す
# compose/ai.yaml: LLM_MODEL=gemma3-4b-12k
# .env: LLM_NUM_CTX=12288

# 2. 再起動
docker compose -f compose/compose.yaml -p alt restart news-creator-backend news-creator
```

### テスト実行

```bash
cd news-creator
SERVICE_SECRET=test-secret uv run pytest app/tests/ --ignore=app/tests/e2e
# 162 passed
```

### 参考文献

- [Pinecone: Chunking Strategies](https://www.pinecone.io/learn/chunking-strategies/) - Map-Reduce チャンクサイズの best practice
- [Google Cloud: Long Document Summarization](https://cloud.google.com/blog/products/ai-machine-learning/long-document-summarization-with-workflows-and-gemini-models) - 階層的要約の手法
