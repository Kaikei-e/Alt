# ADR-000223: Rerank Server メモリ最適化 (FP16 + inference_mode)

## STATUS

Accepted（実装完了・デプロイ・動作確認済み）

## CONTEXT

rerank-server は Apple Silicon Mac 上で `BAAI/bge-reranker-v2-m3`（568M パラメータの CrossEncoder）を MPS バックエンドで動作させている。同一マシンでは Ollama（Augur 用 LLM）も稼働しており、統合メモリを共有している。

### 問題

- rerank-server がモデルを FP32（単精度浮動小数点）でロードしており、モデル重みだけで約 2.3GB の統合メモリを消費
- 推論専用サーバにもかかわらず勾配計算が有効で、不要なバッファがメモリを圧迫
- MPS メモリキャッシュが無制限で、推論後もキャッシュが解放されない
- これらの合計メモリ圧迫により、同一マシン上の Ollama（LLM 推論）の応答速度が低下

### 制約

- API 互換性を完全に維持すること（`POST /v1/rerank`、`GET /health`、`GET /` のリクエスト/レスポンス形式は変更不可）
- rag-orchestrator の reranker client が依存しているため、スコアの精度低下が許容範囲内であること

## DECISION MAKING

推論専用サーバとしての用途に特化し、3つの最適化を同時に適用する。

### 2.1 FP16（半精度）化

CrossEncoder のロード時に `model_kwargs={"dtype": "float16"}` を指定し、モデル重みを半精度でロードする。reranker の出力はソート用の相対スコアであり、FP16 の精度で十分である。

```python
_model = CrossEncoder(
    DEFAULT_MODEL,
    device=DEVICE,
    model_kwargs={"dtype": "float16"},
)
```

### 2.2 勾配無効化 + inference_mode

推論専用のため、モデルを eval モードに設定し全パラメータの `requires_grad` を無効化。さらに推論時は `torch.inference_mode()` コンテキストで実行する。

```python
# モデルロード後
_model.model.eval()
for param in _model.model.parameters():
    param.requires_grad = False

# 推論時
with torch.inference_mode():
    scores = _model.predict(pairs)
```

### 2.3 MPS メモリキャッシュ制限

`PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0` を設定し、MPS のメモリキャッシュ上限を無制限から制限ありに変更。推論後に不要なキャッシュが統合メモリを占有し続けることを防ぐ。

```python
import os
os.environ.setdefault("PYTORCH_MPS_HIGH_WATERMARK_RATIO", "0.0")
```

この環境変数は `import torch` より前に設定する必要がある。

## RESULTS

### 変更ファイル

| ファイル | 変更内容 |
|---------|---------|
| `rerank-server/rerank_server.py` | FP16 化、勾配無効化、inference_mode 適用、MPS キャッシュ制限 |

### 変更しないファイル

| ファイル | 理由 |
|---------|------|
| `rag-orchestrator/internal/adapter/rag_augur/reranker_client.go` | API 契約に変更なし |
| `rerank-server/requirements.txt` | 依存パッケージの追加・変更なし |

### メモリ改善

| 項目 | Before | After |
|------|--------|-------|
| モデル重み | ~2.3GB (FP32) | ~1.1GB (FP16) |
| 勾配バッファ | 有効（メモリ確保） | 無効（解放済み） |
| MPS キャッシュ | 無制限 | 制限あり |

### 動作確認

| 検証項目 | 結果 |
|---------|------|
| `GET /health` | `{"status":"ok","device":"mps","model":"BAAI/bge-reranker-v2-m3"}` |
| `POST /v1/rerank` | 正常応答、スコアが FP16 精度で返却 |
| 起動ログ | deprecation warning なし、正常起動 |

## PROS

1. モデル重みのメモリ使用量が約 50% 削減され、同一マシン上の LLM 推論サービスへのメモリ圧迫が緩和された
2. 勾配バッファの解放と inference_mode により、推論時の一時メモリ確保も最小化された
3. API 互換性を完全に維持しており、クライアント側の変更は不要
4. reranker のスコアはソート用の相対値であるため、FP16 の精度低下は実用上無視できる

## CONS, TRADEOFF

1. FP16 によりスコアの数値精度が FP32 比で低下するが、reranker の用途（候補の相対順序付け）では問題にならない
2. `PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0` はキャッシュを積極的に解放するため、連続リクエスト時のキャッシュヒット率が低下し、レイテンシがわずかに増加する可能性がある。メモリ削減とのトレードオフとして許容する
3. `os.environ.setdefault` を `import torch` より前に配置する必要があり、import 順序に制約がある

## APPENDIX

- 対象モデル: `BAAI/bge-reranker-v2-m3`（568M パラメータ、CrossEncoder ベース）
- sentence-transformers v5.2+ では `model_kwargs` に `dtype` キーを使用する（`torch_dtype` は deprecated）
- Apple Silicon の統合メモリアーキテクチャでは、MPS デバイス上のモデル重みは `ps` の RSS には計上されない。実際のメモリ消費は `vm_stat` やアクティビティモニタの「メモリ」タブで確認する
