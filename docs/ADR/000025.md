# Ryzen AI 9 HX370 iGPU (Radeon 890M) によるハードウェア加速の有効化

## ステータス

採択（Accepted） - 2025年12月30日

## コンテキスト

iGPUを搭載したマシンにおいて、Ollamaによる推論を高速化するため、iGPUをDockerコンテナ内で活用する必要があった。

### 課題

1.  **推論速度**: CPUのみの推論では、20Bクラスのモデル（gpt-oss:20b等）のレスポンスが遅い。
2.  **デバイスアクセス制限**: Dockerコンテナ内のデフォルトユーザー（ollama-user）には、`/dev/kfd` および `/dev/dri/renderD128` へのアクセス権限がない。
3.  **GIDの不安定性**: ホストOS側の `render` グループのGIDは、環境や再起動によって変わる可能性があり、Dockerfile内で固定GIDを指定するのは堅牢ではない。

## 決定事項

### 1. Vulkanによるハードウェア加速の採用

AMD iGPU（RDNA 3.5）をサポートするため、OllamaのVulkanバックエンド（`OLLAMA_VULKAN=1`）を使用する。

### 2. 動的なGID検出と権限付与の自動化

コンテナ起動時にホストのGPUデバイスのGIDを検出し、動的に権限を設定する仕組みを導入した。

- **gosuの導入**: rootから一般ユーザーへの安全な権限委譲のために `gosu` をインストール。
- **entrypoint.shの拡張**:
    - 起動時に `/dev/dri/renderXXXX` および `/dev/kfd` のGIDを `stat` コマンドで検出。
    - 検出されたGIDで一時的なグループ（`render-host`）を作成。
    - `ollama-user` をそのグループに追加。
    - `gosu` を用いて、残りのプロセスを `ollama-user` として実行。

### 3. Docker Composeの構成

`compose.augur.yaml` において以下の設定を維持/確認：
- デバイスマウント: `/dev/kfd`, `/dev/dri`
- 環境変数: `OLLAMA_VULKAN=1`

## 結果

### ポジティブな影響

1.  **完全なGPUオフロード**: `gpt-oss:20b` モデルの全レイヤー（25/25）がGPUにオフロードされるようになった。
2.  **大容量VRAM活用**: iGPUながら **32.0 GiB** の利用可能メモリが認識され、大規模モデルのロードが可能になった。
3.  **環境耐性**: ホスト側のGIDが変更されても、コンテナを再起動するだけで自動的に追従する。
4.  **セキュリティ**: 必要なセットアップのみrootで行い、メインのOllamaプロセスは一般ユーザーで実行される。

### ネガティブな影響 / トレードオフ

1.  **起動時間の僅かな増加**: 起動時にパッケージの確認や権限設定を行うため、数秒のオーバーヘッドが発生する。
2.  **コンテナ権限**: 権限設定のために、一瞬だけコンテナがrootで起動する必要がある（最終的には一般ユーザーにドロップされる）。

## 参考文献

- [Ollama Vulkan Support](https://ollama.com/library)
- [AMD ROCm/Vulkan Docker Documentation](https://rocm.docs.amd.com/en/latest/deploy/docker.html)

## メタデータ

- **作成日:** 2025年12月30日
- **関連ADR:**
  - ADR 000023: RAGプロンプト最適化（高速化の背景）
- **影響を受けるコンポーネント:**
  - knowledge-augur
  - knowledge-embedder
