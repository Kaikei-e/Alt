# LLM モデルルーティング無効化による COLD_START 解消

## ADR's STATUS

Accepted

## CONTEXT

### 背景

LLM 生成速度が期待値の 10-100 倍遅い状態が発生していた。

| 指標 | 観測値 | 期待値 |
|------|--------|--------|
| tokens/sec | 0.32-6.33 | 50-100 |
| load_duration | 0.2-7.67s | 0s |
| COLD_START | 毎リクエスト | なし |

### 根本原因

`MODEL_ROUTING_ENABLED=true` により、リクエストごとにトークン数に応じて 8k/12k/16k モデル間の切り替えが発生していた。Ollama は `OLLAMA_MAX_LOADED_MODELS=1` のため、モデル切り替え時に毎回 VRAM から旧モデルをアンロードし、新モデルをディスクからロードしていた。

```
8k モデル → 12k モデル: load_duration=7.67s (COLD_START)
12k モデル → 8k モデル: load_duration=0.28s (COLD_START)
```

### ADR 140 + ADR 145 の設計意図

```
OLLAMA_NUM_PARALLEL=2 で 8k × 2 スロット構成:
├── RT スロット (1): フロントエンドストリーミング用
└── BE スロット (1): バッチ処理用
```

- **VRAM 予算**: 8GB VRAM 内で収まる 7.1GB 構成
- **モデル**: `gemma3-4b-8k` を両スロットで共有
- **切り替え禁止**: 同一モデルを使用して COLD_START 回避

### 検討した選択肢

| 選択肢 | 説明 | 採用 |
|--------|------|------|
| A. MAX_LOADED_MODELS 増加 | 複数モデルを VRAM に常駐 | 不採用: VRAM 不足 (8GB) |
| B. ルーティング無効化 | 単一モデル固定で切り替えなし | **採用** |
| C. KV キャッシュ量子化 | VRAM 使用量削減 | 不採用: 安定性懸念 |

## DECISION MAKING

### 方針

**モデルルーティング無効化 + 単一モデル固定**

1. `MODEL_ROUTING_ENABLED=false` でルーティングロジックをバイパス
2. `LLM_MODEL=gemma3-4b-8k` で使用モデルを固定
3. 大きな入力は階層的要約 (hierarchical summarization) で対応

### ルーティングロジック

```python
# model_router.py:56-58
if not self.config.model_routing_enabled:
    # Routing disabled, use default model
    return self.config.model_name, self.config.llm_num_ctx
```

`MODEL_ROUTING_ENABLED=false` の場合、トークン数に関係なく `LLM_MODEL` で指定されたモデルを使用する。

## RESULTS, EFFECTS

### 変更ファイル一覧

| ファイル | 変更内容 |
|----------|----------|
| `compose/ai.yaml:101` | `LLM_MODEL=gemma3-4b-8k` (環境変数参照を削除) |
| `compose/ai.yaml:102` | `MODEL_ROUTING_ENABLED=false` |

### 変更前後

```yaml
# Before
- LLM_MODEL=${LLM_MODEL:-gemma3-4b-8k}
- MODEL_ROUTING_ENABLED=true

# After
- LLM_MODEL=gemma3-4b-8k
- MODEL_ROUTING_ENABLED=false
```

### 既に実施済みの関連変更

| ファイル | 変更内容 |
|----------|----------|
| `compose/recap.yaml:106-107` | `RECAP_BATCH_SUMMARY_CHUNK_SIZE=10` (25 から削減) |

### 測定結果

修正後の LLM 生成パフォーマンス:

| 指標 | Before | After |
|------|--------|-------|
| tokens/sec | 0.32-6.33 | 50-65 |
| load_duration | 0.2-7.67s | 0.19s |
| decode tok/s | 不安定 | 64-65 |
| COLD_START | 毎リクエスト | 初回のみ |

### PROS

1. **COLD_START 解消**: モデル切り替えがなくなり、ディスクからのロードが発生しない
2. **安定した性能**: 50-65 tok/s の安定した生成速度
3. **シンプルな運用**: 単一モデル構成で管理が容易

### CONS, TRADEOFF

1. **コンテキスト制限**: 8k トークンを超える入力は階層的要約が必要
2. **柔軟性の低下**: 大きな入力への動的対応ができない

## APPENDIX

### 検証手順

```bash
# 1. コンテナ再起動
docker compose -f compose/compose.yaml -p alt up -d news-creator

# 2. 環境変数確認
docker exec news-creator env | grep -E "(MODEL_ROUTING|LLM_MODEL)"
# 期待値:
# LLM_MODEL=gemma3-4b-8k
# MODEL_ROUTING_ENABLED=false

# 3. ロード済みモデル確認
curl -s http://localhost:11435/api/ps | jq '.models[].name'
# 期待値: "gemma3-4b-8k" のみ

# 4. COLD_START 警告が消えることを確認
docker compose -f compose/compose.yaml -p alt logs -f news-creator 2>&1 | \
  grep -E "(COLD_START|model.*loading)"
```

### 関連 ADR

- ADR 140: 8k × 2 スロット構成の設計
- ADR 145: RT/BE スロット分離

### 参考資料

- [Ollama Model Loading](https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-keep-a-model-loaded-in-memory)
- [VRAM Management](https://github.com/ollama/ollama/issues/3513)
