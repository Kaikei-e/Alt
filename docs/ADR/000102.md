# 意味的評価基盤の導入（BERTScore、ハルシネーション検出、トピック一貫性）

## ADR'S STATUS

Accepted

## CONTEXT

### 背景

7days Recap の品質評価において、既存の ROUGE ベースの評価指標では要約品質を適切に測定できていなかった。特に日本語コンテンツでは、同義語や言い換え表現に対する評価が不正確であり、ハルシネーション（幻覚生成）の検出も行えていなかった。

### 問題

1. **ROUGE の限界**: 表層的な n-gram 一致に依存し、意味的類似性を捉えられない
2. **ハルシネーション未検出**: 生成された要約がソーステキストに基づいているか検証する手段がない
3. **クラスタ品質評価の欠如**: HDBSCAN によるクラスタリング結果の一貫性を定量評価できない
4. **LLM 評価の未導入**: 人間の判断に近い多面的評価（一貫性、流暢性、関連性）ができない

### 実装前の状態

- 評価指標: ROUGE-1, ROUGE-2, ROUGE-L のみ
- ハルシネーション検出: なし
- クラスタ品質: シルエットスコアのみ
- 人間評価相関: 低い

## DECISION MAKING

### 決定

recap-subworker に 4 つの評価モジュールを新規追加する。

#### 1. BERTScore (semantic_eval.py)

言語別に最適化された BERT モデルを使用した意味的類似度評価。

```python
class SemanticEvaluator:
    MODEL_MAP = {
        "ja": "cl-tohoku/bert-base-japanese-v3",
        "en": "microsoft/deberta-xlarge-mnli",
    }

    def compute_bert_score(
        self,
        candidates: list[str],
        references: list[str],
        lang: Literal["ja", "en"] = "ja",
    ) -> BERTScoreResult:
        # Precision, Recall, F1 を返却
```

**選定理由**:
- 日本語: tohoku-nlp/bert-base-japanese-v3 は mBERT より日本語タスクで高精度
- 英語: DeBERTa-xlarge-MNLI は NLI タスクで SOTA

#### 2. ハルシネーション検出 (hallucination_detector.py)

NLI（自然言語推論）ベースの検出。要約文が元テキストに含意されているかを検証。

```python
class HallucinationDetector:
    DEFAULT_MODEL = "tasksource/ModernBERT-base-nli"

    def detect(
        self,
        summary: str,
        source_sentences: list[str],
        threshold: float = 0.5,
    ) -> HallucinationResult:
        # Max-pooling: 各要約文に対し、最も高い含意スコアを採用
```

**アルゴリズム**:
1. 要約を文単位に分割
2. 各要約文に対し、全ソース文との NLI スコアを計算
3. 最大含意スコアが閾値未満なら「非サポート」と判定

#### 3. G-Eval (g_eval.py)

LLM-as-Judge パターンによる多面的評価。Chain-of-Thought で評価理由も生成。

**評価軸**:
- Coherence（一貫性）
- Consistency（整合性）
- Fluency（流暢性）
- Relevance（関連性）

#### 4. トピック一貫性評価 (coherence.py)

C_V / NPMI 指標によるクラスタ品質評価。人間評価との相関が最も高い C_V を採用。

```python
class TopicCoherenceEvaluator:
    def compute_coherence(
        self,
        clusters: dict[int, list[str]],
        texts: list[str],
        lang: Literal["ja", "en"] = "en",
    ) -> CoherenceResult:
        # クラスタごとの代表語を抽出し、一貫性を計算
```

## RESULTS, EFFECTS

### 変更ファイル

| ファイル | 種別 | 内容 |
|----------|------|------|
| `recap-subworker/recap_subworker/services/semantic_eval.py` | 新規 | BERTScore 評価 |
| `recap-subworker/recap_subworker/services/hallucination_detector.py` | 新規 | NLI ハルシネーション検出 |
| `recap-subworker/recap_subworker/services/g_eval.py` | 新規 | LLM-as-Judge 評価 |
| `recap-subworker/recap_subworker/services/coherence.py` | 新規 | トピック一貫性評価 |
| `recap-subworker/pyproject.toml` | 修正 | 依存追加 (bert-score, gensim) |

### 効果

- 意味的類似度の正確な測定
- ハルシネーション率の定量化（目標: < 5%）
- クラスタ品質の客観的評価
- 人間評価との高い相関

### PROS

1. **言語対応**: 日本語・英語それぞれに最適化されたモデル
2. **段階的導入**: 各モジュールは独立しており、段階的に有効化可能
3. **オプショナル依存**: bert-score / transformers / gensim が未インストールでもエラーにならない
4. **バッチ処理対応**: 大量データの効率的な処理が可能

### CONS, TRADEOFF

1. **GPU メモリ**: BERTScore / NLI モデルは GPU メモリを消費（バッチサイズで調整可能）
2. **レイテンシ**: 評価処理に追加時間が必要
3. **モデルサイズ**: 初回実行時にモデルダウンロードが発生

## APPENDIX

### 参考資料

- [BERTScore Paper](https://arxiv.org/abs/1904.09675)
- [tohoku-nlp/bert-base-japanese-v3](https://github.com/cl-tohoku/bert-japanese)
- [HaluGate: Token-Level Detection](https://blog.vllm.ai/2025/12/14/halugate.html)
- [G-Eval: Definitive Guide](https://www.confident-ai.com/blog/g-eval-the-definitive-guide)
- [C_V Topic Coherence](https://towardsdatascience.com/c%E1%B5%A5-topic-coherence-explained-fc70e2a85227/)
