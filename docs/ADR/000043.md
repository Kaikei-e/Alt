# LLM Summary サービス パフォーマンス評価: Gemma3:4b + RTX 4060 8GB

## ステータス

**承認済み（Accepted）** - 2026年1月

## コンテキスト

### 背景

LLM ベースの要約サービスにおいて、コンシューマー GPU (RTX 4060 8GB VRAM) 環境での性能特性を定量評価する必要があった。主な評価観点は以下の通り：

- **TTFT (Time To First Token)**: ユーザー体験に直結する初回応答時間
- **Decode 速度**: 生成速度 (tokens/sec)
- **VRAM 制約**: 8GB という制限下での動作可能範囲

### 評価対象構成

| コンポーネント | 仕様 |
|---------------|------|
| GPU | NVIDIA GeForce RTX 4060 |
| VRAM | 8GB (8188 MiB) |
| LLM | Gemma3:4b (Q4_K_M 量子化) |
| 推論エンジン | Ollama |
| Context Window | 16K / 60K (バケットシステム) |

### 既存アーキテクチャ

#### 1. Model Bucket System

```
┌─────────────────┐      ┌─────────────────┐
│  16K Context    │      │  60K Context    │
│  (~5GB VRAM)    │      │  (~7GB VRAM)    │
│  keep_alive=24h │      │  keep_alive=15m │
└────────┬────────┘      └────────┬────────┘
         │                        │
         └────────┬───────────────┘
                  │
         ┌────────▼────────┐
         │  Model Router   │
         │  (Size-based)   │
         └─────────────────┘
```

**設計意図:**
- 入力サイズに応じて適切なコンテキスト長を自動選択
- 60K は VRAM 節約のため短い keep_alive (15分)
- 16K は頻繁に使用されるため長い keep_alive (24時間)

#### 2. Hierarchical Summarization (Map-Reduce)

```
Large Input (>200K chars)
         │
    ┌────▼────┐
    │  Split  │  ← HIERARCHICAL_CHUNK_MAX_CHARS
    └────┬────┘
         │
   ┌─────┼─────┐
   ▼     ▼     ▼
[Chunk1][Chunk2][Chunk3]  ← Map Phase (並列処理)
   │     │     │
   └─────┼─────┘
         │
    ┌────▼────┐
    │ Reduce  │  ← 中間要約を統合
    └────┬────┘
         │
    Final Summary
```

**設計意図:**
- 60K context を超える入力に対応
- Map フェーズで並列処理により高速化
- 16K context のみで大規模入力を処理可能

#### 3. VRAM 最適化設定

```python
# Ollama 設定
num_batch = 1024      # RTX 4060 最適化
temperature = 0.15    # 低温で安定出力
repeat_penalty = 1.15 # 繰り返し防止
num_predict = 1200    # 最大生成トークン
```

## 意思決定

### 決定: 現行構成の維持 + 60K Context への変更

#### ベンチマーク手法

1. **テストケース設計**

| ケース | 入力サイズ | プロンプトトークン | 使用モデル |
|--------|-----------|-------------------|-----------|
| small | ~500 chars | ~300 tokens | 16K |
| medium | ~2,000 chars | ~700 tokens | 16K |
| large | ~7,000 chars | ~2,000 tokens | 16K |
| xl | ~30,000 chars | ~8,000 tokens | 60K |

2. **計測手法**
   - 各ケース 5-10 回反復
   - Warmup 1-2 回実施
   - P50 (中央値) を主要指標として採用

3. **目標値設定**

| メトリクス | 目標値 | 根拠 |
|-----------|--------|------|
| TTFT | < 2秒 | ユーザー体験 (知覚遅延の閾値) |
| Decode 速度 | > 40 tok/s | RTX 4060 期待値 (Web 調査) |
| Prefill 速度 | > 500 tok/s | GPU アクセラレーション下限 |

#### 計測結果

| ケース | TTFT P50 | Decode tok/s | Latency P50 | 判定 |
|--------|----------|--------------|-------------|------|
| small | 0.25s | 61.4 | 5.86s | PASS |
| medium | 0.56s | 60.5 | 8.34s | PASS |
| large | 1.23s | 51.3 | 10.96s | PASS |
| xl (60K) | - | - | - | VRAM 制約 |

#### VRAM 使用量分析

```
gemma3-4b-16k ロード時:
├── Model Weights: ~3.3GB
├── KV Cache (16K): ~2.0GB
└── Total: ~5.3GB / 8GB

gemma3-4b-60k ロード時 (推定):
├── Model Weights: ~3.3GB
├── KV Cache (60K): ~4-5GB (16K の 4倍)
└── Total: ~7-8GB → 8GB VRAM でギリギリ動作
```

#### 決定事項

1. **16K Context を標準使用** (変更なし)
   - 全テストケースで目標値クリア
   - VRAM 使用量 5.3GB で十分な余裕

2. **80K → 60K Context への変更** (実施済み)
   - 80K context は 8GB VRAM で OOM リスクが高いため 60K に変更
   - `HIERARCHICAL_THRESHOLD_CHARS` を 200,000 → 150,000 に引き下げ推奨
   - より積極的に Map-Reduce を適用

3. **現行パラメータの維持**
   - `num_batch=1024`: 適切なバッチサイズ
   - `temperature=0.15`: 安定した出力品質
   - `keep_alive` 設定: VRAM 管理に効果的

### 修正ファイル

| ファイル | 変更内容 |
|----------|----------|
| `config/config.py` | `HIERARCHICAL_THRESHOLD_CHARS` のデフォルト値変更 (推奨) |

## 結果

### PROS

1. **目標値達成**
   - TTFT: 0.25-1.23s (目標 <2s)
   - Decode: 51-61 tok/s (目標 >40)
   - 16K 範囲内で十分な実用性

2. **VRAM 効率**
   - 5.3GB / 8GB = 65% 使用率
   - 残り 2.7GB で OS/他プロセス用に余裕

3. **コスト効率**
   - RTX 4060 (~$300) で本番運用可能
   - クラウド GPU 不要

4. **安定性**
   - OOM 未発生 (16K 範囲内)
   - 一貫した応答時間

### CONS, TRADEOFF

1. **60K Context 制限**
   - 大規模入力は Map-Reduce 必須
   - 単一パスでの処理は不可

2. **スケーラビリティ制限**
   - 単一 GPU 構成
   - 並列リクエスト処理は制限的

3. **モデルサイズ制約**
   - 4B パラメータが上限
   - 7B/12B モデルは VRAM 不足

## 付録

### ベンチマーク環境詳細

```yaml
Hardware:
  GPU: NVIDIA GeForce RTX 4060
  VRAM: 8188 MiB
  Driver: 575.57.08
  CUDA: 12.9

Software:
  Ollama: latest
  Model: gemma3:4b (Q4_K_M)
  Python: 3.11+
```

### パフォーマンスチューニング参考値

| パラメータ | 推奨値 | 効果 |
|-----------|--------|------|
| `num_batch` | 1024 | RTX 4060 最適 |
| `temperature` | 0.1-0.2 | 安定出力 |
| `repeat_penalty` | 1.1-1.2 | 繰り返し防止 |
| `keep_alive` (16K) | 24h | VRAM ホット維持 |
| `keep_alive` (60K) | 15m | 積極的アンロード |

### 今後の改善案

1. **QAT モデル評価**
   - Google 公式 Quantization-Aware Trained 版
   - 精度劣化 54% 削減の報告あり

2. **Flash Attention 有効化**
   - `OLLAMA_FLASH_ATTENTION=1`
   - KV Cache 効率化の可能性

3. **vLLM 移行検討**
   - 高並列環境での TTFT 改善
   - Continuous Batching サポート

### 参考資料

- [Ollama VRAM Requirements Guide](https://localllm.in/blog/ollama-vram-requirements-for-local-llms)
- [Gemma 3 QAT Models - Google Developers Blog](https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/)
- [RTX 4060 Ollama Benchmark](https://www.databasemart.com/blog/ollama-gpu-benchmark-rtx4060)
- [Ollama vs vLLM Performance Benchmarking](https://developers.redhat.com/articles/2025/08/08/ollama-vs-vllm-deep-dive-performance-benchmarking)
- [Text Summarization with LLMs - Prompt Engineering Guide](https://www.promptingguide.ai/prompts/text-summarization)
