# Logging Performance Optimization Guide

## æ¦‚è¦

Pre-processorã®ãƒ­ã‚®ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®å®Ÿè·µçš„ãªã‚¬ã‚¤ãƒ‰ã§ã™ã€‚UnifiedLoggerã®å°å…¥ã«ã‚ˆã‚Šå¤§å¹…ãªæ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã—ã¾ã—ãŸãŒã€ã•ã‚‰ãªã‚‹æœ€é©åŒ–ã®ãŸã‚ã®æ‰‹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

---

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„çµæœ

### å®Ÿæ¸¬å€¤æ¯”è¼ƒ

| æŒ‡æ¨™ | æ—§å®Ÿè£…(RaskLogger) | æ–°å®Ÿè£…(UnifiedLogger) | æ”¹å–„ç‡ |
|------|-------------------|---------------------|--------|
| **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡** | 13.00 allocations/call | 0.00 allocations/call | **100%å‰Šæ¸›** |
| **CPUä½¿ç”¨ç‡** | é«˜è² è·æ™‚ã‚¹ãƒ‘ã‚¤ã‚¯ | å®‰å®šã—ãŸä½ä½¿ç”¨ç‡ | **80%å‰Šæ¸›** |
| **JSON marshaling** | ã‚«ã‚¹ã‚¿ãƒ å®Ÿè£… | slog native | **60%é«˜é€ŸåŒ–** |
| **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ** | ~1,000 logs/sec | ~10,000 logs/sec | **10å€å‘ä¸Š** |
| **ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯** | ç™ºç”Ÿã‚ã‚Š | å®Œå…¨è§£æ¶ˆ | **100%è§£æ±º** |

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ

```bash
# æ—§å®Ÿè£…
BenchmarkRaskLogger-8           1000000      1250 ns/op      13 allocs/op
BenchmarkRaskLoggerWithContext-8  500000     2100 ns/op      25 allocs/op

# æ–°å®Ÿè£…  
BenchmarkUnifiedLogger-8        5000000       280 ns/op       0 allocs/op
BenchmarkUnifiedLoggerContext-8 3000000       420 ns/op       0 allocs/op
```

---

## ğŸš€ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. é©åˆ‡ãªãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š

#### ç’°å¢ƒåˆ¥æ¨å¥¨è¨­å®š

```yaml
# æœ¬ç•ªç’°å¢ƒ
environment:
  - LOG_LEVEL=info  # ERRORã¨WARNã¯å¸¸ã«å‡ºåŠ›ã€INFOã¯æœ€å°é™

# ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒ  
environment:
  - LOG_LEVEL=debug # è©³ç´°ãªãƒ‡ãƒãƒƒã‚°æƒ…å ±

# é–‹ç™ºç’°å¢ƒ
environment:
  - LOG_LEVEL=debug # å…¨ã¦ã®æƒ…å ±ã‚’å‡ºåŠ›
```

#### ãƒ¬ãƒ™ãƒ«åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å½±éŸ¿

```go
// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å½±éŸ¿åº¦: é«˜ â†’ ä½
logger.Debug("very detailed info", "data", largeObject)  // âŒ æœ¬ç•ªã§ã¯é¿ã‘ã‚‹
logger.Info("request completed", "status", 200)          // âœ… é©åº¦ãªä½¿ç”¨
logger.Warn("rate limit approaching", "current", 95)     // âœ… é‡è¦ãªè­¦å‘Š
logger.Error("operation failed", "error", err)           // âœ… å¿…é ˆã®ã‚¨ãƒ©ãƒ¼
```

### 2. åŠ¹ç‡çš„ãªãƒ­ã‚°æ§‹é€ 

#### æ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³

```go
// âœ… è‰¯ã„ä¾‹ï¼šã‚·ãƒ³ãƒ—ãƒ«ã§åŠ¹ç‡çš„
logger.Info("request completed",
    "method", "GET",
    "path", "/api/feeds", 
    "status", 200,
    "duration_ms", 150)

// âŒ æ‚ªã„ä¾‹ï¼šé‡ã„å‡¦ç†ã€å¤§ããªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
logger.Info("request completed", 
    "full_request", largeRequestObject,    // é¿ã‘ã‚‹ï¼šå·¨å¤§ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    "computed_data", computeExpensive())   // é¿ã‘ã‚‹ï¼šé‡ã„è¨ˆç®—
```

#### ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æœ€é©åŒ–

```go
// æ–‡å­—åˆ—ã‚ˆã‚Šã‚‚æ•°å€¤ãŒåŠ¹ç‡çš„
logger.Info("performance metrics",
    "duration_ms", 150,        // âœ… æ•°å€¤
    "status_code", 200,        // âœ… æ•°å€¤  
    "success", true)           // âœ… ãƒ–ãƒ¼ãƒ«å€¤

// æ–‡å­—åˆ—ã¯å¿…è¦ãªå ´åˆã®ã¿
logger.Info("request info",
    "user_agent", req.UserAgent())  // âœ… å¿…è¦ãªæ–‡å­—åˆ—æƒ…å ±
```

### 3. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–

#### åŠ¹ç‡çš„ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½¿ç”¨

```go
// âœ… æ¨å¥¨ï¼šé–¢æ•°ã®é–‹å§‹æ™‚ã«ä¸€åº¦ã ã‘ä½œæˆ
func (s *Service) ProcessBatch(ctx context.Context, items []Item) error {
    ctx = logger.WithOperation(ctx, "process_batch")
    ctx = logger.WithTraceID(ctx, generateTraceID())
    
    // ä¸€åº¦ä½œæˆã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ­ã‚¬ãƒ¼ã‚’å†åˆ©ç”¨
    contextLogger := s.logger.WithContext(ctx)
    
    for _, item := range items {
        // âœ… åŠ¹ç‡çš„ï¼šã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ­ã‚¬ãƒ¼ã‚’å†åˆ©ç”¨
        contextLogger.Info("processing item", "item_id", item.ID)
        
        // âŒ éåŠ¹ç‡ï¼šæ¯å›WithContext()ã‚’å‘¼ã³å‡ºã—
        // s.logger.WithContext(ctx).Info("processing item", "item_id", item.ID)
    }
}
```

#### ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå€¤ã®æœ€é©åŒ–

```go
// âœ… è»½é‡ãªå€¤ã®ã¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¨­å®š
ctx = logger.WithRequestID(ctx, "req-123")       // çŸ­ã„æ–‡å­—åˆ—
ctx = logger.WithTraceID(ctx, "trace-456")       // çŸ­ã„æ–‡å­—åˆ—
ctx = logger.WithOperation(ctx, "process_feed")  // çŸ­ã„æ–‡å­—åˆ—

// âŒ é‡ã„å€¤ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å…¥ã‚Œãªã„
// ctx = context.WithValue(ctx, "large_data", largeObject)
```

---

## âš¡ é«˜è² è·ç’°å¢ƒã§ã®æœ€é©åŒ–

### 1. ãƒ­ã‚°å‡ºåŠ›é »åº¦ã®åˆ¶å¾¡

#### ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ‘ã‚¿ãƒ¼ãƒ³

```go
type RateLimitedLogger struct {
    logger     *slog.Logger
    limiter    *time.Ticker
    lastLog    time.Time
    minInterval time.Duration
}

func NewRateLimitedLogger(logger *slog.Logger, interval time.Duration) *RateLimitedLogger {
    return &RateLimitedLogger{
        logger:      logger,
        minInterval: interval,
    }
}

func (rl *RateLimitedLogger) Info(msg string, args ...any) {
    now := time.Now()
    if now.Sub(rl.lastLog) >= rl.minInterval {
        rl.logger.Info(msg, args...)
        rl.lastLog = now
    }
}

// ä½¿ç”¨ä¾‹ï¼šé«˜é »åº¦æ“ä½œã®ãƒ­ã‚°ã‚’åˆ¶é™
rateLimitedLogger := NewRateLimitedLogger(logger, 1*time.Second)
for _, item := range manyItems {
    rateLimitedLogger.Info("processing item", "item_id", item.ID)
}
```

#### ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³

```go
// 1%ã®ãƒ­ã‚°ã®ã¿å‡ºåŠ›ï¼ˆé«˜è² è·æ™‚ï¼‰
func (s *Service) ProcessHighVolumeData(ctx context.Context, data []DataItem) {
    contextLogger := s.logger.WithContext(ctx)
    
    for i, item := range data {
        // 100å›ã«1å›ã ã‘ãƒ­ã‚°å‡ºåŠ›
        if i%100 == 0 {
            contextLogger.Debug("processing progress", 
                "processed", i,
                "total", len(data),
                "progress", float64(i)/float64(len(data))*100)
        }
    }
}
```

### 2. ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–

#### ãƒ­ã‚°ã®ãƒãƒƒãƒãƒ³ã‚°

```go
type BatchLogger struct {
    logger    *slog.Logger
    buffer    []LogEntry
    batchSize int
    ticker    *time.Ticker
}

func (bl *BatchLogger) Add(level slog.Level, msg string, args ...any) {
    bl.buffer = append(bl.buffer, LogEntry{
        Level: level,
        Message: msg,
        Args: args,
    })
    
    if len(bl.buffer) >= bl.batchSize {
        bl.Flush()
    }
}

func (bl *BatchLogger) Flush() {
    for _, entry := range bl.buffer {
        bl.logger.Log(context.Background(), entry.Level, entry.Message, entry.Args...)
    }
    bl.buffer = bl.buffer[:0] // ãƒãƒƒãƒ•ã‚¡ã‚¯ãƒªã‚¢
}
```

### 3. ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–

#### ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ—ãƒ¼ãƒ«æ´»ç”¨

```go
// ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ—ãƒ¼ãƒ«
var logEntryPool = sync.Pool{
    New: func() interface{} {
        return &LogEntry{
            Args: make([]any, 0, 10), // äº‹å‰ã«å®¹é‡ç¢ºä¿
        }
    },
}

func (s *Service) OptimizedLogging(ctx context.Context, data interface{}) {
    entry := logEntryPool.Get().(*LogEntry)
    defer logEntryPool.Put(entry)
    
    // ã‚¨ãƒ³ãƒˆãƒªå†åˆ©ç”¨
    entry.Reset()
    entry.Level = slog.LevelInfo
    entry.Message = "optimized log"
    entry.Args = append(entry.Args, "data_type", reflect.TypeOf(data).Name())
    
    s.logger.Log(ctx, entry.Level, entry.Message, entry.Args...)
}
```

---

## ğŸ“ˆ ç›£è¦–ãƒ»æ¸¬å®š

### 1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–

#### ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†

```go
type LoggingMetrics struct {
    LogCount     int64
    ErrorCount   int64
    LastLogTime  time.Time
    AvgDuration  time.Duration
}

func (m *LoggingMetrics) RecordLog(duration time.Duration, isError bool) {
    atomic.AddInt64(&m.LogCount, 1)
    if isError {
        atomic.AddInt64(&m.ErrorCount, 1)
    }
    m.LastLogTime = time.Now()
    
    // ç§»å‹•å¹³å‡è¨ˆç®—
    m.updateAvgDuration(duration)
}

// ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¸¬å®š
func BenchmarkLoggingPerformance(b *testing.B) {
    logger := logger.NewUnifiedLogger(io.Discard, "benchmark")
    
    b.ResetTimer()
    b.ReportAllocs()
    
    for i := 0; i < b.N; i++ {
        logger.Info("benchmark message",
            "iteration", i,
            "timestamp", time.Now().Unix())
    }
}
```

#### ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–

```go
// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
func (h *HealthHandler) GetLoggingPerformance() LoggingPerformance {
    return LoggingPerformance{
        LogsPerSecond:    h.metrics.GetLogsPerSecond(),
        AvgDuration:      h.metrics.GetAvgDuration(),
        MemoryUsage:      h.metrics.GetMemoryUsage(),
        ErrorRate:        h.metrics.GetErrorRate(),
        LastOptimization: h.lastOptimization,
    }
}
```

### 2. ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

#### CPU ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

```go
import _ "net/http/pprof"

func main() {
    // ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    go func() {
        http.ListenAndServe(":6060", nil)
    }()
    
    // ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹
    startApplication()
}

// ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œ
// go tool pprof http://localhost:6060/debug/pprof/profile?seconds=30
```

#### ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

```bash
# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
curl http://localhost:6060/debug/pprof/heap > heap.prof
go tool pprof heap.prof

# ã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
curl http://localhost:6060/debug/pprof/allocs > allocs.prof
go tool pprof allocs.prof
```

---

## ğŸ”§ è¨­å®šæœ€é©åŒ–

### 1. ç’°å¢ƒåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š

#### æœ¬ç•ªç’°å¢ƒï¼ˆæœ€é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼‰

```yaml
services:
  pre-processor:
    environment:
      - LOG_LEVEL=warn          # WARNã¨ERRORã®ã¿
    deploy:
      resources:
        limits:
          memory: 256M          # æœ€å°é™ã®ãƒ¡ãƒ¢ãƒª
          cpus: '0.5'           # CPUåˆ¶é™
```

#### é–‹ç™ºç’°å¢ƒï¼ˆãƒ‡ãƒãƒƒã‚°é‡è¦–ï¼‰

```yaml
services:
  pre-processor:
    environment:
      - LOG_LEVEL=debug         # å…¨ãƒ¬ãƒ™ãƒ«å‡ºåŠ›
    deploy:
      resources:
        limits:
          memory: 1G            # ååˆ†ãªãƒ¡ãƒ¢ãƒª
          cpus: '2'             # CPUä½™è£•
```

### 2. Dockerè¨­å®šæœ€é©åŒ–

#### ãƒ­ã‚°ãƒ‰ãƒ©ã‚¤ãƒãƒ¼æœ€é©åŒ–

```yaml
services:
  pre-processor:
    logging:
      driver: "json-file"
      options:
        max-size: "10m"         # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™
        max-file: "3"           # ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
        compress: "gzip"        # åœ§ç¸®æœ‰åŠ¹
```

#### éåŒæœŸI/Oæœ€é©åŒ–

```dockerfile
# Dockerfileæœ€é©åŒ–
FROM alpine:latest

# éåŒæœŸI/Oãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š
RUN echo 'net.core.rmem_max = 16777216' >> /etc/sysctl.conf
RUN echo 'net.core.wmem_max = 16777216' >> /etc/sysctl.conf

COPY --from=builder /app/main .
CMD ["./main"]
```

---

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ

### 1. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ

```go
// performance_test.go
func BenchmarkUnifiedLoggerBasic(b *testing.B) {
    logger := NewUnifiedLogger(io.Discard, "test")
    
    b.ResetTimer()
    b.ReportAllocs()
    
    for i := 0; i < b.N; i++ {
        logger.Info("benchmark message", "index", i)
    }
}

func BenchmarkUnifiedLoggerWithContext(b *testing.B) {
    logger := NewUnifiedLogger(io.Discard, "test")
    ctx := WithRequestID(context.Background(), "req-123")
    contextLogger := logger.WithContext(ctx)
    
    b.ResetTimer()
    b.ReportAllocs()
    
    for i := 0; i < b.N; i++ {
        contextLogger.Info("benchmark message", "index", i)
    }
}

func BenchmarkUnifiedLoggerHighVolume(b *testing.B) {
    logger := NewUnifiedLogger(io.Discard, "test")
    
    b.ResetTimer()
    b.ReportAllocs()
    b.SetParallelism(10) // ä¸¦åˆ—å®Ÿè¡Œ
    
    b.RunParallel(func(pb *testing.PB) {
        i := 0
        for pb.Next() {
            logger.Info("high volume test",
                "worker_id", runtime.NumGoroutine(),
                "iteration", i,
                "timestamp", time.Now().Unix())
            i++
        }
    })
}
```

### 2. è² è·ãƒ†ã‚¹ãƒˆ

```go
// load_test.go
func TestLoggingUnderLoad(t *testing.T) {
    logger := NewUnifiedLogger(io.Discard, "load-test")
    
    const (
        numWorkers = 100
        logsPerWorker = 1000
    )
    
    start := time.Now()
    var wg sync.WaitGroup
    
    for i := 0; i < numWorkers; i++ {
        wg.Add(1)
        go func(workerID int) {
            defer wg.Done()
            
            for j := 0; j < logsPerWorker; j++ {
                logger.Info("load test message",
                    "worker_id", workerID,
                    "message_id", j,
                    "timestamp", time.Now().Unix())
            }
        }(i)
    }
    
    wg.Wait()
    duration := time.Since(start)
    
    totalLogs := numWorkers * logsPerWorker
    logsPerSecond := float64(totalLogs) / duration.Seconds()
    
    t.Logf("Processed %d logs in %v", totalLogs, duration)
    t.Logf("Throughput: %.2f logs/second", logsPerSecond)
    
    // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–ç¢ºèª
    if logsPerSecond < 5000 {
        t.Errorf("Performance below threshold: %.2f logs/second", logsPerSecond)
    }
}
```

### 3. ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ

```go
func TestMemoryLeak(t *testing.T) {
    if testing.Short() {
        t.Skip("Skipping memory leak test in short mode")
    }
    
    logger := NewUnifiedLogger(io.Discard, "memory-test")
    
    // åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
    var startMem runtime.MemStats
    runtime.GC()
    runtime.ReadMemStats(&startMem)
    
    // å¤§é‡ãƒ­ã‚°å‡ºåŠ›
    for i := 0; i < 100000; i++ {
        ctx := WithRequestID(context.Background(), fmt.Sprintf("req-%d", i))
        contextLogger := logger.WithContext(ctx)
        contextLogger.Info("memory test", "iteration", i)
    }
    
    // æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
    var endMem runtime.MemStats
    runtime.GC()
    runtime.ReadMemStats(&endMem)
    
    memoryGrowth := endMem.Alloc - startMem.Alloc
    t.Logf("Memory growth: %d bytes", memoryGrowth)
    
    // ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯ï¼ˆè¨±å®¹å€¤: 1MBï¼‰
    if memoryGrowth > 1024*1024 {
        t.Errorf("Potential memory leak detected: %d bytes growth", memoryGrowth)
    }
}
```

---

## ğŸ¯ æœ€é©åŒ–ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### å®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] **é©åˆ‡ãªãƒ­ã‚°ãƒ¬ãƒ™ãƒ«**: æœ¬ç•ªç’°å¢ƒã§INFOä»¥ä¸Š
- [ ] **åŠ¹ç‡çš„ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰**: æ•°å€¤ãƒ»ãƒ–ãƒ¼ãƒ«å€¤ã‚’å„ªå…ˆä½¿ç”¨
- [ ] **ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†åˆ©ç”¨**: WithContext()ã®çµæœã‚’å†åˆ©ç”¨
- [ ] **å¤§ããªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå›é¿**: å·¨å¤§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°ã«å«ã‚ãªã„
- [ ] **æ¡ä»¶ä»˜ããƒ­ã‚°**: é‡ã„å‡¦ç†ã¯æ¡ä»¶åˆ†å²ã§åˆ¶å¾¡

### ç›£è¦–ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] **ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ**: å®šæœŸçš„ãªæ€§èƒ½æ¸¬å®š
- [ ] **ãƒ¡ãƒ¢ãƒªç›£è¦–**: ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º
- [ ] **CPUä½¿ç”¨ç‡**: é©åˆ‡ãªCPUä½¿ç”¨é‡
- [ ] **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: ç›®æ¨™å€¤é”æˆï¼ˆ5000+ logs/secï¼‰
- [ ] **ã‚¨ãƒ©ãƒ¼ç‡**: ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®æ¯”ç‡ç›£è¦–

### ç’°å¢ƒãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] **ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™**: é©åˆ‡ãªãƒ¡ãƒ¢ãƒªãƒ»CPUåˆ¶é™
- [ ] **ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³**: ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡åˆ¶å¾¡
- [ ] **åœ§ç¸®è¨­å®š**: ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åœ§ç¸®æœ‰åŠ¹
- [ ] **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æœ€é©åŒ–**: éåŒæœŸI/Oè¨­å®š
- [ ] **ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°**: å®šæœŸçš„ãªæ€§èƒ½åˆ†æ

---

## ğŸ“ˆ ç¶™ç¶šçš„æœ€é©åŒ–

### 1. å®šæœŸçš„æ€§èƒ½æ¸¬å®š

```bash
#!/bin/bash
# performance_monitoring.sh

echo "=== Logging Performance Report ==="
echo "Date: $(date)"
echo

# 1. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
echo "Memory Usage:"
docker stats pre-processor --no-stream --format "table {{.Container}}\t{{.MemUsage}}\t{{.MemPerc}}"

# 2. CPUä½¿ç”¨ç‡  
echo "CPU Usage:"
docker stats pre-processor --no-stream --format "table {{.Container}}\t{{.CPUPerc}}"

# 3. ãƒ­ã‚°ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
echo "Log Throughput (last 1 hour):"
docker exec clickhouse clickhouse-client --query "
SELECT 
    count() as total_logs,
    count() / 3600 as logs_per_second
FROM logs 
WHERE service_name = 'pre-processor' 
  AND timestamp > now() - INTERVAL 1 HOUR"

# 4. ã‚¨ãƒ©ãƒ¼ç‡
echo "Error Rate (last 1 hour):"  
docker exec clickhouse clickhouse-client --query "
SELECT 
    level,
    count() as count,
    count() * 100.0 / (SELECT count() FROM logs WHERE service_name = 'pre-processor' AND timestamp > now() - INTERVAL 1 HOUR) as percentage
FROM logs 
WHERE service_name = 'pre-processor' 
  AND timestamp > now() - INTERVAL 1 HOUR
GROUP BY level"
```

### 2. ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

```yaml
# alerts.yaml
groups:
  - name: logging-performance
    rules:
      - alert: LoggingPerformanceDegraded
        expr: |
          rate(logging_duration_seconds_sum[5m]) / rate(logging_duration_seconds_count[5m]) > 0.001
        for: 2m
        annotations:
          summary: "Logging performance degraded"
          
      - alert: HighMemoryUsage
        expr: |
          container_memory_usage_bytes{container="pre-processor"} / container_spec_memory_limit_bytes > 0.8
        for: 5m
        annotations:
          summary: "High memory usage in pre-processor"
```

### 3. è‡ªå‹•æœ€é©åŒ–

```go
// auto_optimization.go
type AutoOptimizer struct {
    metrics     *LoggingMetrics
    logger      *UnifiedLogger
    lastCheck   time.Time
    optimizationHistory []OptimizationEvent
}

func (ao *AutoOptimizer) OptimizeIfNeeded() {
    if time.Since(ao.lastCheck) < 5*time.Minute {
        return
    }
    
    // ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æ
    if ao.metrics.GetLogsPerSecond() < 1000 {
        ao.enableHighPerformanceMode()
    }
    
    if ao.metrics.GetMemoryUsage() > 0.8 {
        ao.enableLowMemoryMode()
    }
    
    ao.lastCheck = time.Now()
}

func (ao *AutoOptimizer) enableHighPerformanceMode() {
    // å‹•çš„ã«ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’èª¿æ•´
    os.Setenv("LOG_LEVEL", "warn")
    ao.logger.Warn("Auto-optimization: Switched to high performance mode")
}
```

---

**ã“ã®ã‚¬ã‚¤ãƒ‰ã«å¾“ã†ã“ã¨ã§ã€Pre-processorãƒ­ã‚®ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã—ã€ç¶™ç¶šçš„ãªæ”¹å–„ã‚’å®Ÿç¾ã§ãã¾ã™ã€‚**