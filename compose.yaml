# ALT Platform Docker Compose Configuration
#
# Changes made:
# - Updated pre-processor to use multi-stage Dockerfile with pre-compiled Go binary
# - Added comments for volume definitions
# - Aligned with Kubernetes deployment configuration
#
# Usage:
#   docker-compose up -d                    # Start all services
#   docker-compose up -d --profile logging  # Start with logging services
#   docker-compose down --profile logging   # Stop with logging services
#
networks:
  alt-network:

volumes:
  # Persistent storage volumes for data persistence
  db_data:                    # PostgreSQL database data
  kratos_db_data:             # Kratos PostgreSQL database data
  meili_data:                 # MeiliSearch index data
  rask_log_aggregator_data:   # Rask log aggregator data
  clickhouse_data:            # ClickHouse database data
  news_creator_models:        # Ollama models for news-creator
x-rask-env: &rask-env
  environment:
    RASK_CONFIG: |
      endpoint = "http://rask-log-aggregator:9600/v1/aggregate"
      batch_size = 1
      flush_interval_ms = 500
      buffer_capacity = 100000
      log_level = "info"

x-rask-forwarder-env: &rask-forwarder-env
  environment:
    DOCKER_HOST: "unix:///var/run/docker.sock"
    LOG_LEVEL: "info"
    RUST_LOG: "info"
    RASK_ENDPOINT: "http://rask-log-aggregator:9600/v1/aggregate"
    BATCH_SIZE: "1000"
    FLUSH_INTERVAL_MS: "500"
    BUFFER_CAPACITY: "100000"

x-shared-env-file: &shared-env-file
  env_file:
    - .env

services:
  nginx:
    image: nginx:latest
    restart: always
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
    depends_on:
      alt-frontend:
        condition: service_healthy
      alt-backend:
        condition: service_healthy
      kratos:
        condition: service_healthy
      auth-hub:
        condition: service_started
    networks:
      - alt-network
    # Resource limits for better performance
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
    # Increase file descriptor limits
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    tty: true
    labels:
      - rask.group=alt-frontend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  alt-frontend:
    <<: *shared-env-file
    build:
      context: ./alt-frontend
      dockerfile: Dockerfile.frontend
      x-bake: &buildkit
        platforms: ["linux/amd64"]
      args:
        - NEXT_PUBLIC_API_BASE_URL=/api
        - API_URL=http://alt-backend:9000
        - NEXT_PUBLIC_IDP_ORIGIN=${NEXT_PUBLIC_IDP_ORIGIN}
        - NEXT_PUBLIC_KRATOS_PUBLIC_URL=${NEXT_PUBLIC_KRATOS_PUBLIC_URL}
        - NEXT_PUBLIC_APP_ORIGIN=${NEXT_PUBLIC_APP_ORIGIN}
        - NEXT_PUBLIC_RETURN_TO_DEFAULT=${NEXT_PUBLIC_RETURN_TO_DEFAULT}
        - KRATOS_INTERNAL_URL=${KRATOS_INTERNAL_URL}
        - KRATOS_PUBLIC_URL=${KRATOS_PUBLIC_URL}
    environment:
      - NEXT_PUBLIC_API_BASE_URL=/api
      - API_URL=http://alt-backend:9000
      - NEXT_PUBLIC_APP_ORIGIN=${NEXT_PUBLIC_APP_ORIGIN}
      - NEXT_PUBLIC_RETURN_TO_DEFAULT=${NEXT_PUBLIC_RETURN_TO_DEFAULT}
      - KRATOS_INTERNAL_URL=${KRATOS_INTERNAL_URL}
      - KRATOS_PUBLIC_URL=${KRATOS_PUBLIC_URL}
      - NODE_ENV=production
      - PORT=3000
    ports:
      - "3000:3000"
    restart: always
    networks:
      - alt-network
    # Resource limits
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:3000/api/health | grep -q '\"status\":\"ok\"' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    tty: true

  alt-backend:
    <<: *shared-env-file
    build:
      context: ./alt-backend
      dockerfile: Dockerfile.backend
    restart: always
    ports:
      - "9000:9000"
    networks:
      - alt-network
    volumes:
      - .env:/app/.env
    environment:
      - DB_HOST=db
      - DB_PORT=5432
      - DB_USER=${POSTGRES_USER}
      - DB_PASSWORD=${POSTGRES_PASSWORD}
      - DB_NAME=${POSTGRES_DB}
      - DB_SSL_MODE=prefer
    depends_on:
      db:
        condition: service_healthy
      migrate:
        condition: service_completed_successfully
    # Resource limits for backend
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '4.0'
        reservations:
          memory: 2G
          cpus: '2.0'
    # Increase file descriptor limits for backend
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9000/v1/health || exit 1"]
      interval: 20s
      timeout: 10s
      retries: 5
      start_period: 60s
    tty: true
    labels:
      - rask.group=alt-backend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  news-creator:
    profiles:
      - ollama
    <<: *shared-env-file
    build:
      context: ./news-creator
      dockerfile: Dockerfile.creator
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_HOME=/home/ollama-user/.ollama
      - OLLAMA_MODELS=/home/ollama-user/.ollama
      - LLM_SERVICE_URL=${LLM_SERVICE_URL:-http://127.0.0.1:11435}
      - SERVICE_SECRET=${SERVICE_SECRET}
      - LLM_MODEL=${LLM_MODEL:-gemma3:4b}
      - LLM_TIMEOUT_SECONDS=${LLM_TIMEOUT_SECONDS:-180}
      - LLM_NUM_CTX=${LLM_NUM_CTX:-16384}
      - LLM_NUM_PREDICT=${LLM_NUM_PREDICT:-1200}
      - SUMMARY_NUM_PREDICT=${SUMMARY_NUM_PREDICT:-500}
    volumes:
      - news_creator_models:/home/ollama-user/.ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    networks:
      - alt-network
    depends_on:
      news-creator-volume-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 60s
    labels:
      - rask.group=news-creator
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  news-creator-volume-init:
    profiles:
      - ollama
    build:
      context: ./news-creator
      dockerfile: Dockerfile.creator
    entrypoint: ["/bin/sh", "-c"]
    command: ["chown -R 2000:2000 /home/ollama-user/.ollama"]
    user: "0:0"
    volumes:
      - news_creator_models:/home/ollama-user/.ollama
    networks:
      - alt-network
    restart: "no"
    labels:
      - rask.group=news-creator-init

  pre-processor:
    profiles:
      - ollama
    <<: *shared-env-file
    # Updated to use multi-stage build with pre-compiled Go binary
    build:
      context: ./pre-processor
      dockerfile: Dockerfile
    environment:
      - DB_HOST=db
      - DB_PORT=5432
      - DB_NAME=${DB_NAME}
      - PRE_PROCESSOR_DB_USER=${PRE_PROCESSOR_DB_USER}
      - PRE_PROCESSOR_DB_PASSWORD=${PRE_PROCESSOR_DB_PASSWORD}
      - DB_SSL_MODE=prefer
      - LOG_LEVEL=info
      - SERVICE_NAME=pre-processor
    depends_on:
      db:
        condition: service_healthy
      news-creator:
        condition: service_healthy
    restart: always
    ports:
      - "9200:9200"
    networks:
      - alt-network
    tty: true
    labels:
      - rask.group=pre-processor
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
  search-indexer:
    <<: *shared-env-file
    build:
      context: ./search-indexer
      dockerfile: Dockerfile.search-indexer
    environment:
      - DB_HOST=db
      - DB_PORT=5432
      - DB_NAME=${DB_NAME}
      - DB_SSL_MODE=prefer
      - SEARCH_INDEXER_DB_USER=${SEARCH_INDEXER_DB_USER}
      - SEARCH_INDEXER_DB_PASSWORD=${SEARCH_INDEXER_DB_PASSWORD}
      - MEILISEARCH_HOST=${MEILISEARCH_HOST}
      - MEILISEARCH_API_KEY=${MEILI_MASTER_KEY}
    depends_on:
      db:
        condition: service_healthy
      meilisearch:
        condition: service_healthy
    networks:
      - alt-network
    ports:
      - "9300:9300"
    tty: true
    labels:
      - rask.group=search-indexer
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
  tag-generator:
    <<: *shared-env-file
    build:
      context: ./tag-generator
      dockerfile: Dockerfile.tag-generator
    networks:
      - alt-network
    ports:
      - "9400:9400"
    environment:
      - DB_HOST=db
      - DB_PORT=5432
      - DB_NAME=${DB_NAME}
      - DB_TAG_GENERATOR_USER=${DB_TAG_GENERATOR_USER}
      - DB_TAG_GENERATOR_PASSWORD=${DB_TAG_GENERATOR_PASSWORD}
      # Force CPU-only to prevent GPU memory issues
      - CUDA_VISIBLE_DEVICES=""
    tty: true
    depends_on:
      db:
        condition: service_healthy
    restart: unless-stopped
    labels:
      - rask.group=tag-generator
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
  meilisearch:
      image: getmeili/meilisearch:v1.15.2
      restart: unless-stopped
      environment:
        MEILI_MASTER_KEY: ${MEILI_MASTER_KEY}
        MEILI_ENV: "production"
      ports:
        - "7700:7700"
      volumes:
        - meili_data:/meili_data   # persistent index storage
      networks:
        - alt-network
      healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:7700/health"]
        interval: 10s
        timeout: 5s
        retries: 5
      deploy:
        resources:
          limits:
            memory: 15G
          reservations:
            memory: 8G
      tty: true
      labels:
        - rask.group=meilisearch
      logging:
        driver: "json-file"
        options:
          max-size: "10m"
          max-file: "3"
  rask-log-aggregator:
    build:
      context: ./rask-log-aggregator
      dockerfile: Dockerfile.rask-log-aggregator
    restart: unless-stopped
    ports:
      - "9600:9600"
    networks:
      - alt-network
    tty: true
    labels:
      - rask.group=rask-log-aggregator
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      clickhouse:
        condition: service_healthy
    environment:
      APP_CLICKHOUSE_HOST: clickhouse
      APP_CLICKHOUSE_PORT: 8123
      APP_CLICKHOUSE_USER: ${CLICKHOUSE_USER}
      APP_CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
      APP_CLICKHOUSE_DATABASE: ${CLICKHOUSE_DB}
  clickhouse:
    image: clickhouse/clickhouse-server:25.6
    restart: unless-stopped
    environment:
      CLICKHOUSE_DB: ${CLICKHOUSE_DB:-rask_logs}
      CLICKHOUSE_USER: ${CLICKHOUSE_USER:-rask_user}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-rask_password}
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    ports:
      - "8123:8123"
      - "9009:9000"
    volumes:
      - ./clickhouse/init:/docker-entrypoint-initdb.d
      - clickhouse_data:/var/lib/clickhouse
    networks:
      - alt-network
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/ping"]
      interval: 10s
      timeout: 5s
      retries: 3
  db:
    image: postgres:16-alpine
    restart: always
    command: >
      postgres
      -c config_file=/etc/postgresql/postgresql.conf
      -c hba_file=/etc/postgresql/pg_hba.conf
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      DB_HOST: ${DB_HOST}
      DB_PORT: ${DB_PORT}
      DB_USER: ${DB_USER}
      DB_PASSWORD: ${DB_PASSWORD}
      DB_NAME: ${DB_NAME}
      PRE_PROCESSOR_DB_USER: ${PRE_PROCESSOR_DB_USER}
      PRE_PROCESSOR_DB_PASSWORD: ${PRE_PROCESSOR_DB_PASSWORD}
      DB_TAG_GENERATOR_USER: ${DB_TAG_GENERATOR_USER}
      DB_TAG_GENERATOR_PASSWORD: ${DB_TAG_GENERATOR_PASSWORD}
      SEARCH_INDEXER_DB_USER: ${SEARCH_INDEXER_DB_USER}
      SEARCH_INDEXER_DB_PASSWORD: ${SEARCH_INDEXER_DB_PASSWORD}
    volumes:
      - db_data:/var/lib/postgresql/data
      - ./db/init:/docker-entrypoint-initdb.d:rw
      - ./docker/postgres/postgresql.conf:/etc/postgresql/postgresql.conf:ro
      - ./docker/postgres/pg_hba.conf:/etc/postgresql/pg_hba.conf:ro
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - alt-network
    tty: true
    labels:
      - rask.group=db
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  kratos-db:
    image: postgres:16-alpine
    restart: always
    command: >
      postgres
      -c config_file=/etc/postgresql/postgresql.conf
      -c hba_file=/etc/postgresql/pg_hba.conf
    environment:
      POSTGRES_DB: ${KRATOS_DB_NAME:-kratos}
      POSTGRES_USER: ${KRATOS_DB_USER:-kratos_user}
      POSTGRES_PASSWORD: ${KRATOS_DB_PASSWORD:-kratos_password}
    volumes:
      - kratos_db_data:/var/lib/postgresql/data
      - ./kratos-db/init:/docker-entrypoint-initdb.d:rw
      - ./docker/postgres/postgresql.conf:/etc/postgresql/postgresql.conf:ro
      - ./docker/postgres/pg_hba.conf:/etc/postgresql/pg_hba.conf:ro
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${KRATOS_DB_USER:-kratos_user} -d ${KRATOS_DB_NAME:-kratos}"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - alt-network
    tty: true
    labels:
      - rask.group=kratos-db
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  migrate:
    <<: *shared-env-file
    build:
      context: ./migrations-atlas
      dockerfile: docker/Dockerfile
    container_name: db_migrator
    command: ["apply"]
    environment:
      DATABASE_URL: "postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}?sslmode=disable&search_path=public"
      MIGRATE_MAX_RETRIES: "12"
      MIGRATE_RETRY_INTERVAL: "5"
      MIGRATE_BASELINE_VERSION: ${MIGRATE_BASELINE_VERSION:-}
      ATLAS_ENV: ${ATLAS_ENV:-kubernetes}
      ATLAS_REVISIONS_SCHEMA: "public"
    volumes:
      - ./migrations-atlas/migrations:/migrations:ro
    depends_on:
      db:
        condition: service_healthy
    networks:
      - alt-network

################### For Rask Log Aggregator ###################
  nginx-logs:
    build:
      context: ./rask-log-forwarder/app
      dockerfile: Dockerfile.rask-log-forwarder
    network_mode: "service:nginx"
    <<: *rask-forwarder-env
    environment:
      TARGET_SERVICE: "${NGINX_TARGET:-nginx}"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    group_add:
      - "${DOCKER_GROUP_ID:-984}"  # docker group (fallback to 984)
    restart: unless-stopped
    profiles:
      - logging

  alt-backend-logs:
    build:
      context: ./rask-log-forwarder/app
      dockerfile: Dockerfile.rask-log-forwarder
    network_mode: "service:alt-backend"
    <<: *rask-forwarder-env
    environment:
      TARGET_SERVICE: "${ALT_BACKEND_TARGET:-alt-backend}"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    group_add:
      - "${DOCKER_GROUP_ID:-984}"  # docker group (fallback to 984)
    restart: unless-stopped
    profiles:
      - logging

  tag-generator-logs:
    build:
      context: ./rask-log-forwarder/app
      dockerfile: Dockerfile.rask-log-forwarder
    network_mode: "service:tag-generator"
    <<: *rask-forwarder-env
    environment:
      TARGET_SERVICE: "tag-generator"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    group_add:
      - "${DOCKER_GROUP_ID:-984}"  # docker group (fallback to 984)
    restart: unless-stopped
    profiles:
      - logging

  pre-processor-logs:
    build:
      context: ./rask-log-forwarder/app
      dockerfile: Dockerfile.rask-log-forwarder
    network_mode: "service:pre-processor"
    <<: *rask-forwarder-env
    environment:
      TARGET_SERVICE: "pre-processor"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    group_add:
      - "${DOCKER_GROUP_ID:-984}"  # docker group (fallback to 984)
    restart: unless-stopped
    profiles:
      - logging

  search-indexer-logs:
    build:
      context: ./rask-log-forwarder/app
      dockerfile: Dockerfile.rask-log-forwarder
    network_mode: "service:search-indexer"
    <<: *rask-forwarder-env
    environment:
      TARGET_SERVICE: "search-indexer"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    group_add:
      - "${DOCKER_GROUP_ID:-984}"  # docker group (fallback to 984)
    restart: unless-stopped
    profiles:
      - logging

  news-creator-logs:
    build:
      context: ./rask-log-forwarder/app
      dockerfile: Dockerfile.rask-log-forwarder
    network_mode: "service:news-creator"
    <<: *rask-forwarder-env
    environment:
      TARGET_SERVICE: "news-creator"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    group_add:
      - "${DOCKER_GROUP_ID:-984}"  # docker group (fallback to 984)
    restart: unless-stopped
    profiles:
      - logging

  meilisearch-logs:
    build:
      context: ./rask-log-forwarder/app
      dockerfile: Dockerfile.rask-log-forwarder
    network_mode: "service:meilisearch"
    <<: *rask-forwarder-env
    environment:
      TARGET_SERVICE: "meilisearch"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    restart: unless-stopped
    profiles:
      - logging

  db-logs:
    build:
      context: ./rask-log-forwarder/app
      dockerfile: Dockerfile.rask-log-forwarder
    network_mode: "service:db"
    <<: *rask-forwarder-env
    environment:
      TARGET_SERVICE: "db"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    restart: unless-stopped
    profiles:
      - logging

  kratos-migrate:
    image: oryd/kratos:v1.2.0
    command: ["migrate", "sql", "-e", "--yes", "--config", "/etc/config/kratos/kratos.yml"]
    environment:
      - DSN=postgres://${KRATOS_DB_USER:-kratos_user}:${KRATOS_DB_PASSWORD:-kratos_password}@${KRATOS_DB_HOST:-kratos-db}:${KRATOS_DB_PORT:-5432}/${KRATOS_DB_NAME:-kratos}?sslmode=${KRATOS_DB_SSLMODE:-disable}
      - KRATOS_DB_USER=${KRATOS_DB_USER:-kratos_user}
      - KRATOS_DB_PASSWORD=${KRATOS_DB_PASSWORD:-kratos_password}
      - KRATOS_DB_HOST=${KRATOS_DB_HOST:-kratos-db}
      - KRATOS_DB_PORT=${KRATOS_DB_PORT:-5432}
      - KRATOS_DB_NAME=${KRATOS_DB_NAME:-kratos}
      - KRATOS_DB_SSLMODE=${KRATOS_DB_SSLMODE:-disable}
    volumes:
      - ./kratos:/etc/config/kratos:ro
    depends_on:
      kratos-db:
        condition: service_healthy
    restart: "no"
    networks:
      - alt-network

  kratos:
    image: oryd/kratos:v1.2.0
    command: ["serve", "all", "--config", "/etc/config/kratos/kratos.yml"]
    ports:
      - "4433:4433"  # public
      - "4434:4434"  # admin
    environment:
      - DSN=postgres://${KRATOS_DB_USER:-kratos_user}:${KRATOS_DB_PASSWORD:-kratos_password}@${KRATOS_DB_HOST:-kratos-db}:${KRATOS_DB_PORT:-5432}/${KRATOS_DB_NAME:-kratos}?sslmode=${KRATOS_DB_SSLMODE:-disable}
      - KRATOS_DB_USER=${KRATOS_DB_USER:-kratos_user}
      - KRATOS_DB_PASSWORD=${KRATOS_DB_PASSWORD:-kratos_password}
      - KRATOS_DB_HOST=${KRATOS_DB_HOST:-kratos-db}
      - KRATOS_DB_PORT=${KRATOS_DB_PORT:-5432}
      - KRATOS_DB_NAME=${KRATOS_DB_NAME:-kratos}
      - KRATOS_DB_SSLMODE=${KRATOS_DB_SSLMODE:-disable}
      - KRATOS_PUBLIC_URL=${KRATOS_PUBLIC_URL:-http://localhost/ory}
      - KRATOS_ADMIN_URL=${KRATOS_ADMIN_URL:-http://kratos-admin:4434/}
      - KRATOS_CORS_ALLOWED_ORIGIN_1=${KRATOS_CORS_ALLOWED_ORIGIN_1:-http://localhost:3000}
      - KRATOS_CORS_ALLOWED_ORIGIN_2=${KRATOS_CORS_ALLOWED_ORIGIN_2:-http://localhost}
      - KRATOS_DEFAULT_BROWSER_RETURN_URL=${KRATOS_DEFAULT_BROWSER_RETURN_URL:-http://localhost:3000/}
      - KRATOS_ALLOWED_RETURN_URL_1=${KRATOS_ALLOWED_RETURN_URL_1:-http://localhost:3000/}
      - KRATOS_ALLOWED_RETURN_URL_2=${KRATOS_ALLOWED_RETURN_URL_2:-http://localhost:3000/home}
      - KRATOS_ALLOWED_RETURN_URL_3=${KRATOS_ALLOWED_RETURN_URL_3:-http://localhost:3000/auth/login}
      - KRATOS_LOGIN_UI_URL=${KRATOS_LOGIN_UI_URL:-http://localhost:3000/auth/login}
      - KRATOS_REGISTRATION_UI_URL=${KRATOS_REGISTRATION_UI_URL:-http://localhost:3000/auth/register}
      - KRATOS_ERROR_UI_URL=${KRATOS_ERROR_UI_URL:-http://localhost:3000/auth/error}
      - KRATOS_SETTINGS_UI_URL=${KRATOS_SETTINGS_UI_URL:-http://localhost:3000/auth/settings}
      - KRATOS_RECOVERY_UI_URL=${KRATOS_RECOVERY_UI_URL:-http://localhost:3000/auth/recovery}
      - KRATOS_VERIFICATION_UI_URL=${KRATOS_VERIFICATION_UI_URL:-http://localhost:3000/auth/verification}
      - KRATOS_COOKIE_SECRET=${KRATOS_COOKIE_SECRET:-kratos_cookie_secret_32_chars_xx}
      - KRATOS_CIPHER_SECRET=${KRATOS_CIPHER_SECRET:-kratos_cipher_secret_32_chars}
    volumes:
      - ./kratos:/etc/config/kratos:ro
    depends_on:
      kratos-db:
        condition: service_healthy
      kratos-migrate:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://127.0.0.1:4434/admin/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - alt-network

  auth-hub:
    build:
      context: ./auth-hub
      dockerfile: Dockerfile
    environment:
      - KRATOS_URL=http://kratos:4433
      - PORT=8888
      - CACHE_TTL=5m
    depends_on:
      kratos:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - alt-network
    deploy:
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
    labels:
      - rask.group=alt-auth
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

